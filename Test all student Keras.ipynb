{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import T. Kipf's GCN implementation\n",
    "# https://github.com/tkipf/keras-gcn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import keras\n",
    "sys.path.append('./keras-gcn/')\n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_hdf('history_small.hdf', key='hist') \n",
    "A = pd.read_pickle('adjacency_small.pkl') \n",
    "A = A[sorted(A.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((878, 290), (878, 878))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,  10.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X==1] = 10 #10 #valid\n",
    "X[X==0] = 1 #not valid\n",
    "X[X==-100] = 0 #missing\n",
    "X = X.astype(float)\n",
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = X.values.flatten()\n",
    "y[y==0]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find index of student that answered to most questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123482.0\n"
     ]
    }
   ],
   "source": [
    "for col in X:\n",
    "    if (len(X[col][X[col]!=0])> 500):\n",
    "        print(col)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X[123482]!=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([80]),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X.columns==123482.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output : flattened input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = X.values.ravel(order = 'F') #len(y) =n*m : 878*290 #rempli en colonne de sorte que \n",
    "                                        #per user_id, the responses to all the exercise_id (valid/not valid/missing)\n",
    "#np.where(y!=0)[0].shape #8821 non null entries (1 or 10)\n",
    "#y[y==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for col in X : \n",
    "    a = pd.get_dummies(X[col])\n",
    "    if (1 not in a.columns) & (10 in a.columns):\n",
    "        a[1] = np.zeros(a.shape[0],dtype=int)\n",
    "    if (1 in a.columns) & (10 not in a.columns):\n",
    "        a[10] = np.zeros(a.shape[0],dtype=int)\n",
    "    l.append(a[[1,10]])\n",
    "X = pd.concat(l, axis=1, keys=X.columns)\n",
    "X.columns = [str(int(col[0]))+'_'+str(col[1]) for col in X.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = scipy.sparse.csr_matrix(A.values)\n",
    "X = np.asmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 580) (878, 878) (254620,)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> <class 'scipy.sparse.csr.csr_matrix'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, A.shape, y.shape) #n*2m, n*m\n",
    "print(type(X), type(A), type(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def get_splits(y):\n",
    "#    idx_train = range(878)\n",
    "#    idx_val = range(878)#range(878, 1756)\n",
    "#    idx_test = range(878)#range(1756, 2634)\n",
    "#    y_train = np.zeros(len(idx_train), dtype=np.int32)\n",
    "#    y_val = np.zeros(len(idx_val), dtype=np.int32)\n",
    "#    y_test = np.zeros(len(idx_test), dtype=np.int32)\n",
    "#    y_train = y[idx_train]\n",
    "#    y_val = y[idx_val]\n",
    "#    y_test = y[idx_test]\n",
    "#    train_mask = np.array(np.ones(len(range(878))), dtype=np.bool)\n",
    "#    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "#y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILTER = 'localpool'  \n",
    "MAX_DEGREE = 2 \n",
    "SYM_NORM = True  \n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  \n",
    "\n",
    "# Normalize X\n",
    "X = X/X.sum(1).reshape(-1, 1)\n",
    "\n",
    "A_ = preprocess_adj(A, SYM_NORM)\n",
    "support = 1\n",
    "graph = [X, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ = pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.where(y!=0)[0][878:1756].shape\n",
    "#idx_train = np.where(y!=0)[0][:878]\n",
    "#idx_val = np.where(y!=0)[0][878:1756].shape\n",
    "#idx_test = np.where(y!=0)[0][1756:2634].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_train = range(70240,71118)\n",
    "idx_val = range(70240,71118)\n",
    "idx_test = range(70240,71118)\n",
    "\n",
    "y_train = y_[idx_train]\n",
    "y_test = y_[idx_test]\n",
    "y_val = y_[idx_val]\n",
    "\n",
    "#list_ind = np.where(y_train!=0)[0]\n",
    "#graph_train = [graph[0][list_ind], graph[1][list_ind][:,list_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y_.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#b = loss_(y_train, y_train)\n",
    "#sess = tf.Session()\n",
    "#type(b.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_(y_true, y_pred):\n",
    "    list_ind = np.where(y_train!=np.nan)[0]\n",
    "    mask = np.zeros((y_train.shape[0],1))\n",
    "    mask[list_ind] = 1\n",
    "    #mask = keras.utils.to_categorical(mask)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss=loss_, optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def accuracy(preds, labels):\n",
    "#    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "#def evaluate_preds(preds, labels, indices):\n",
    "\n",
    "#    split_loss = list()\n",
    "#    split_acc = list()\n",
    "\n",
    "#    for y_split, idx_split in zip(labels, indices):\n",
    "#        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "#        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "\n",
    "#    return split_loss, split_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mask = sample_mask(np.where(y_train!=0)[0], y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "878/878 [==============================] - 1s 636us/step - loss: 0.0481\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 206us/step - loss: 0.0439\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 195us/step - loss: 0.0410\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 202us/step - loss: 0.0383\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 206us/step - loss: 0.0363\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 220us/step - loss: 0.0348\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 210us/step - loss: 0.0339\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 206us/step - loss: 0.0333\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 214us/step - loss: 0.0330\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 201us/step - loss: 0.0329\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 200us/step - loss: 0.0330\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 195us/step - loss: 0.0331\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 199us/step - loss: 0.0332\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 205us/step - loss: 0.0334\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 200us/step - loss: 0.0335\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 204us/step - loss: 0.0335\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 204us/step - loss: 0.0335\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 246us/step - loss: 0.0334\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 274us/step - loss: 0.0334\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 227us/step - loss: 0.0332\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 373us/step - loss: 0.0331\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 202us/step - loss: 0.0330\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 335us/step - loss: 0.0328\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 212us/step - loss: 0.0327\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 416us/step - loss: 0.0326\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 219us/step - loss: 0.0325\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 225us/step - loss: 0.0325\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 211us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 199us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 539us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 198us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 301us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 200us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 209us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 282us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 204us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 478us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 406us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 199us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 390us/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 213us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 195us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 429us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 344us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 255us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 416us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 248us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 246us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 257us/step - loss: 0.0323\n",
      "Epoch 1/1\n",
      "878/878 [==============================] - 0s 250us/step - loss: 0.0323\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-ff97ad1d2153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     model.fit(graph, y_train, sample_weight=train_mask,\n\u001b[0;32m----> 9\u001b[0;31m               batch_size=A.shape[0], epochs=1, shuffle=False, verbose=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "    t = time.time()\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=1)\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "    \n",
    "    # Train / validation scores\n",
    "    #train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "    #                                               [idx_train, idx_val])\n",
    "    #print(\"Epoch: {:04d}\".format(epoch),\n",
    "    #      \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "    #      \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "    #      \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "    #      \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "    #      \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    #if train_val_loss[1] < best_val_loss:\n",
    "    #    best_val_loss = train_val_loss[1]\n",
    "    #    wait = 0\n",
    "    #else:\n",
    "    #    if wait >= PATIENCE:\n",
    "    #        print('Epoch {}: early stopping'.format(epoch))\n",
    "    #        break\n",
    "    #    wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51252425,  0.51201069,  0.51208144,  0.51253653,  0.5118041 ,\n",
       "        0.51229376,  0.51236653,  0.51231217,  0.5128516 ,  0.51212335,\n",
       "        0.51289302,  0.51288599,  0.51230735,  0.51267272,  0.51229095,\n",
       "        0.51287019,  0.51290262,  0.51276314,  0.51281542,  0.51243496,\n",
       "        0.51257396,  0.51234871,  0.51234627,  0.51270264,  0.51264107,\n",
       "        0.51303852,  0.51271826,  0.51289207,  0.51253009,  0.51228434,\n",
       "        0.51204091,  0.51244682,  0.51232362,  0.51215893,  0.51301086,\n",
       "        0.51243871,  0.51278168,  0.51219296,  0.51138586,  0.51297837,\n",
       "        0.51298803,  0.51266664,  0.51276958,  0.5126183 ,  0.51244164,\n",
       "        0.51271659,  0.51268536,  0.51269186,  0.51270908,  0.51299679,\n",
       "        0.51302654,  0.51273096,  0.51258332,  0.51214987,  0.51280242,\n",
       "        0.51203448,  0.51253247,  0.51257855,  0.51229656,  0.51260686,\n",
       "        0.51299316,  0.51301652,  0.51249737,  0.51299036,  0.51263297,\n",
       "        0.51297849,  0.51299918,  0.51243758,  0.51301086,  0.51260942,\n",
       "        0.51280969,  0.51274288,  0.51272547,  0.5119403 ,  0.51197022,\n",
       "        0.51185697,  0.5118522 ,  0.51198083,  0.51195395,  0.51226765,\n",
       "        0.51185066,  0.5118565 ,  0.51197958,  0.51185119,  0.5128572 ,\n",
       "        0.51219398,  0.51241428,  0.51271594,  0.5128665 ,  0.51299697,\n",
       "        0.51263243,  0.51282597,  0.51303595,  0.51268625,  0.51250505,\n",
       "        0.51253647,  0.51271784,  0.51271468,  0.51259255,  0.51302469,\n",
       "        0.51249182,  0.51242465,  0.51287097,  0.51270169,  0.51276135,\n",
       "        0.51291549,  0.5129416 ,  0.51264352,  0.51291233,  0.51263195,\n",
       "        0.51240534,  0.51240355,  0.51292366,  0.51255339,  0.5126096 ,\n",
       "        0.51270372,  0.51288724,  0.513035  ,  0.51270008,  0.51278913,\n",
       "        0.51244926,  0.51200497,  0.51255327,  0.51288891,  0.51245242,\n",
       "        0.51241344,  0.51295829,  0.51210016,  0.51236719,  0.51292825,\n",
       "        0.51260185,  0.51279855,  0.51282656,  0.51286435,  0.51282644,\n",
       "        0.5125069 ,  0.5120092 ,  0.51282793,  0.51233429,  0.51181769,\n",
       "        0.51261312,  0.51263505,  0.51274198,  0.511949  ,  0.51229632,\n",
       "        0.51257664,  0.51251656,  0.51243013,  0.51253515,  0.51189965,\n",
       "        0.51288468,  0.51195335,  0.51257885,  0.51224524,  0.51211238,\n",
       "        0.5120396 ,  0.51202595,  0.51183444,  0.51161873,  0.51216328,\n",
       "        0.51191616,  0.51230216,  0.512187  ,  0.51218623,  0.51219749,\n",
       "        0.51197022,  0.51197976,  0.51209569,  0.51209491,  0.5124197 ,\n",
       "        0.51220256,  0.51212132,  0.51291299,  0.5126043 ,  0.51259559,\n",
       "        0.51243442,  0.5121823 ,  0.51239884,  0.51251221,  0.51285458,\n",
       "        0.51213318,  0.51218688,  0.51236767,  0.5120731 ,  0.51257026,\n",
       "        0.51234108,  0.51199412,  0.51246887,  0.51233429,  0.51272482,\n",
       "        0.5126152 ,  0.51284671,  0.51221913,  0.51212794,  0.51216155,\n",
       "        0.51232058,  0.51217908,  0.51219159,  0.5126766 ,  0.51299673,\n",
       "        0.51301229,  0.51270533,  0.51269329,  0.51273847,  0.51202297,\n",
       "        0.51187915,  0.51250768,  0.51295841,  0.51259923,  0.51265121,\n",
       "        0.51222384,  0.5124045 ,  0.51251888,  0.51267529,  0.51222724,\n",
       "        0.512685  ,  0.51228166,  0.51235497,  0.51284266,  0.51294738,\n",
       "        0.51279515,  0.51297599,  0.51151693,  0.51273018,  0.51165193,\n",
       "        0.51273066,  0.51227403,  0.51209509,  0.51263696,  0.5128786 ,\n",
       "        0.51279402,  0.51260704,  0.51277614,  0.51242286,  0.51268077,\n",
       "        0.51271605,  0.51274133,  0.5129509 ,  0.51252276,  0.51299959,\n",
       "        0.51300144,  0.51291472,  0.51297379,  0.51260561,  0.51227623,\n",
       "        0.51262587,  0.51247656,  0.51295239,  0.51271111,  0.51269037,\n",
       "        0.51256818,  0.51243824,  0.51264769,  0.51211214,  0.51246256,\n",
       "        0.51263636,  0.51237911,  0.51244044,  0.51220757,  0.51293194,\n",
       "        0.51250345,  0.51269448,  0.51192391,  0.51271433,  0.51277119,\n",
       "        0.51267654,  0.5122056 ,  0.5129723 ,  0.51260722,  0.51262105,\n",
       "        0.51277554,  0.51245201,  0.51259583,  0.51258206,  0.51230943,\n",
       "        0.51235217,  0.51301748,  0.512339  ,  0.51267177,  0.51268834,\n",
       "        0.51229149,  0.51289028,  0.51295578,  0.51234311,  0.51273078,\n",
       "        0.51254916,  0.51259387,  0.51284552,  0.51270127,  0.51268184,\n",
       "        0.51243919,  0.51261735,  0.51245445,  0.51289213,  0.51282847,\n",
       "        0.5124076 ,  0.51269817,  0.51280522,  0.51164937,  0.51279896,\n",
       "        0.51254189,  0.5123837 ,  0.51241881,  0.5126161 ,  0.51261091,\n",
       "        0.51274735,  0.512824  ,  0.51269621,  0.51224351,  0.5123167 ,\n",
       "        0.5118947 ,  0.51253718,  0.51185739,  0.51188099,  0.51235485,\n",
       "        0.51235682,  0.51170963,  0.51204455,  0.51231682,  0.51197338,\n",
       "        0.51187259,  0.51169437,  0.51199895,  0.51234114,  0.51177579,\n",
       "        0.51189023,  0.51209694,  0.51248771,  0.51266605,  0.51273853,\n",
       "        0.51274329,  0.51237077,  0.51288199,  0.51247931,  0.51298106,\n",
       "        0.51242381,  0.5126816 ,  0.51273775,  0.51237094,  0.51294172,\n",
       "        0.51258361,  0.51296318,  0.5130133 ,  0.51245779,  0.51194429,\n",
       "        0.51286572,  0.51302159,  0.51302236,  0.51228756,  0.51293069,\n",
       "        0.51216781,  0.51256382,  0.51302069,  0.51204175,  0.51259118,\n",
       "        0.51219445,  0.51290649,  0.51300472,  0.51296026,  0.51276243,\n",
       "        0.51254004,  0.51273835,  0.51273787,  0.51267475,  0.51300585,\n",
       "        0.51273268,  0.51185787,  0.51241189,  0.51226616,  0.51268882,\n",
       "        0.51188779,  0.5126701 ,  0.51203167,  0.51268679,  0.51266122,\n",
       "        0.51244527,  0.51268142,  0.51293987,  0.51249039,  0.5127359 ,\n",
       "        0.51260161,  0.51269555,  0.51250607,  0.51275802,  0.51287359,\n",
       "        0.51279002,  0.51292026,  0.51256686,  0.51246637,  0.51299971,\n",
       "        0.51251078,  0.51260465,  0.51273119,  0.51284403,  0.5124945 ,\n",
       "        0.51249599,  0.51228964,  0.5124433 ,  0.51249874,  0.51224512,\n",
       "        0.51258242,  0.51282203,  0.51244038,  0.5128991 ,  0.51279241,\n",
       "        0.51300472,  0.51281333,  0.5123837 ,  0.51261616,  0.51247764,\n",
       "        0.51285124,  0.51260912,  0.51256126,  0.51239139,  0.51233011,\n",
       "        0.51300257,  0.51274425,  0.51261383,  0.51267004,  0.51133198,\n",
       "        0.51288277,  0.51281005,  0.51268882,  0.51270342,  0.51260144,\n",
       "        0.51176572,  0.51292014,  0.51261222,  0.51288813,  0.51261002,\n",
       "        0.5124222 ,  0.51198798,  0.51231104,  0.51284987,  0.51257175,\n",
       "        0.5119943 ,  0.51181483,  0.51232946,  0.51237077,  0.51214373,\n",
       "        0.51192129,  0.51223308,  0.51190138,  0.5116927 ,  0.51235074,\n",
       "        0.51256949,  0.51288033,  0.51289058,  0.51288402,  0.51238889,\n",
       "        0.51291192,  0.51263678,  0.51270211,  0.5126887 ,  0.5127601 ,\n",
       "        0.51291466,  0.51211905,  0.51223242,  0.51269335,  0.51227945,\n",
       "        0.51024485,  0.51225436,  0.51241904,  0.51120925,  0.51192725,\n",
       "        0.51248074,  0.51283336,  0.51264071,  0.51274908,  0.51219356,\n",
       "        0.51253653,  0.51241952,  0.51227373,  0.51232415,  0.51217753,\n",
       "        0.51241535,  0.5119791 ,  0.51247627,  0.51220512,  0.51198739,\n",
       "        0.51166517,  0.51237935,  0.51224828,  0.51195431,  0.51188582,\n",
       "        0.51194865,  0.51255351,  0.51254368,  0.51296335,  0.512896  ,\n",
       "        0.51238549,  0.51288116,  0.51281118,  0.51262039,  0.51285386,\n",
       "        0.51224768,  0.51192051,  0.51283383,  0.5123952 ,  0.51292336,\n",
       "        0.51251107,  0.51282167,  0.51204085,  0.51259285,  0.51246887,\n",
       "        0.51237571,  0.51257795,  0.51281708,  0.51294726,  0.51250714,\n",
       "        0.51260394,  0.5126732 ,  0.51253957,  0.51245958,  0.51304245,\n",
       "        0.5122658 ,  0.51280665,  0.51283079,  0.51266998,  0.51218832,\n",
       "        0.51252794,  0.51270187,  0.51232606,  0.51275879,  0.51283514,\n",
       "        0.51262814,  0.51282263,  0.51251167,  0.51229507,  0.51243681,\n",
       "        0.51262867,  0.51296687,  0.51237869,  0.51239562,  0.5127241 ,\n",
       "        0.51286626,  0.51270491,  0.51290119,  0.51301646,  0.51259041,\n",
       "        0.51239586,  0.5125407 ,  0.51232958,  0.51276374,  0.51211721,\n",
       "        0.51245576,  0.51175869,  0.51223224,  0.51227933,  0.51213086,\n",
       "        0.51222056,  0.51273459,  0.51163459,  0.51224381,  0.51205462,\n",
       "        0.51190656,  0.51220226,  0.51195806,  0.51228255,  0.51208484,\n",
       "        0.5122807 ,  0.51236862,  0.51252156,  0.5124321 ,  0.51239407,\n",
       "        0.51259583,  0.51256573,  0.51270002,  0.51271039,  0.51110899,\n",
       "        0.51287454,  0.51188534,  0.51223224,  0.51225597,  0.51224995,\n",
       "        0.51224387,  0.51281404,  0.51178777,  0.51217681,  0.51225424,\n",
       "        0.51221406,  0.51222378,  0.51288581,  0.51275909,  0.51287693,\n",
       "        0.51271999,  0.51270002,  0.51244271,  0.51287049,  0.51284939,\n",
       "        0.51279747,  0.51245993,  0.51261181,  0.51251912,  0.51227307,\n",
       "        0.51244551,  0.5127992 ,  0.51221669,  0.51266992,  0.51222718,\n",
       "        0.51123744,  0.51216757,  0.51196271,  0.51146263,  0.512299  ,\n",
       "        0.51219559,  0.51237231,  0.51192033,  0.51214439,  0.51168209,\n",
       "        0.51219702,  0.51198608,  0.51235086,  0.51233947,  0.51255715,\n",
       "        0.51211208,  0.51223528,  0.51128465,  0.5122956 ,  0.51264805,\n",
       "        0.51261705,  0.5128296 ,  0.51225144,  0.51271343,  0.51297158,\n",
       "        0.51256925,  0.5128749 ,  0.51280075,  0.51298946,  0.51236993,\n",
       "        0.51234448,  0.51296598,  0.51235515,  0.51184142,  0.51300955,\n",
       "        0.51252466,  0.51244217,  0.5126546 ,  0.51295215,  0.51265854,\n",
       "        0.51300812,  0.51245034,  0.51262742,  0.5119462 ,  0.51257581,\n",
       "        0.51237798,  0.51242787,  0.51269352,  0.51270807,  0.51226556,\n",
       "        0.51296979,  0.51272601,  0.5126881 ,  0.51293916,  0.51256186,\n",
       "        0.51279318,  0.51271373,  0.51233262,  0.51264733,  0.51235843,\n",
       "        0.51172483,  0.51215661,  0.51298815,  0.51295322,  0.51258945,\n",
       "        0.51253235,  0.51177388,  0.51090544,  0.51152819,  0.51158476,\n",
       "        0.51086533,  0.51099813,  0.5117985 ,  0.51197499,  0.51141453,\n",
       "        0.51191235,  0.51205027,  0.51032287,  0.51192021,  0.51106399,\n",
       "        0.51237911,  0.51161402,  0.51097721,  0.51258594,  0.51205873,\n",
       "        0.51188582,  0.51002926,  0.51183969,  0.51151949,  0.51254362,\n",
       "        0.511693  ,  0.51183909,  0.5126974 ,  0.51214486,  0.51188892,\n",
       "        0.51257753,  0.51262343,  0.51123804,  0.51253605,  0.51206118,\n",
       "        0.51277834,  0.51264191,  0.51275164,  0.51268947,  0.51163143,\n",
       "        0.51195103,  0.51136029,  0.51172423,  0.51199281,  0.51040667,\n",
       "        0.51176423,  0.51185447,  0.51207167,  0.51176465,  0.51230317,\n",
       "        0.51267457,  0.51165611,  0.51281404,  0.51240635,  0.51253647,\n",
       "        0.5102405 ,  0.51255995,  0.51241398,  0.51250315,  0.51186323,\n",
       "        0.51232576,  0.51245737,  0.5121004 ,  0.51270646,  0.51195759,\n",
       "        0.51279581,  0.51300317,  0.51269376,  0.51183456,  0.51194715,\n",
       "        0.51248562,  0.51207191,  0.51266146,  0.5128715 ,  0.51287782,\n",
       "        0.51188493,  0.51218253,  0.51280063,  0.51237214,  0.51213968,\n",
       "        0.51284879,  0.51251447,  0.51205361,  0.51250201,  0.51290482,\n",
       "        0.51289314,  0.51271677,  0.51250225,  0.51256418,  0.51281822,\n",
       "        0.5127598 ,  0.51282108,  0.51185644,  0.51251042,  0.51271904,\n",
       "        0.51034892,  0.51245338,  0.5124231 ,  0.51224381,  0.51227456,\n",
       "        0.51290035,  0.51034832,  0.51161414,  0.51257867,  0.51221073,\n",
       "        0.51283902,  0.51291168,  0.51269567,  0.51289499,  0.51221305,\n",
       "        0.51295006,  0.51298839,  0.51298642,  0.51301748,  0.51280975,\n",
       "        0.51270598,  0.51182294,  0.51249593,  0.51273978,  0.51254731,\n",
       "        0.51216519,  0.51261199,  0.51266521,  0.51253277,  0.51226127,\n",
       "        0.51303422,  0.51272762,  0.51290005,  0.51246774,  0.51271582,\n",
       "        0.51270664,  0.51277316,  0.51293093,  0.51251107,  0.51291156,\n",
       "        0.51034832,  0.51267219,  0.51243246,  0.51191878,  0.51302719,\n",
       "        0.51236045,  0.51034832,  0.51277179,  0.51296616,  0.51255858,\n",
       "        0.51229453,  0.51226312,  0.5128715 ,  0.51301712,  0.51231956,\n",
       "        0.51293117,  0.51271319,  0.51256186,  0.51245505,  0.51292908,\n",
       "        0.51272351,  0.51251644,  0.51220006,  0.51238102,  0.51189488,\n",
       "        0.51293486,  0.51303339,  0.51272666,  0.51269943,  0.51186007,\n",
       "        0.51165462,  0.51263529,  0.51262838,  0.51264161,  0.51251054,\n",
       "        0.51275373,  0.5126521 ,  0.51281059,  0.51251203,  0.51280785,\n",
       "        0.51255715,  0.51284838,  0.51204383,  0.51291829,  0.51203799,\n",
       "        0.51196814,  0.51135856,  0.5119524 ,  0.51158673,  0.51060033,\n",
       "        0.51203877,  0.51145279,  0.50950241,  0.5117653 ,  0.51244783,\n",
       "        0.51222312,  0.51260859,  0.51217598,  0.51241243,  0.51272911,\n",
       "        0.51218545,  0.51217675,  0.51219141,  0.51194263,  0.51224607,\n",
       "        0.5123409 ,  0.51242393,  0.51222122,  0.51245308,  0.51251328,\n",
       "        0.51248288,  0.51228619,  0.51233125,  0.51142782,  0.51119226,\n",
       "        0.51205969,  0.51226377,  0.51276314], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6414 accuracy= 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
