{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import T. Kipf's GCN implementation\n",
    "# https://github.com/tkipf/keras-gcn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import keras\n",
    "sys.path.append('./keras-gcn/')\n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_hdf('history_small.hdf', key='hist') \n",
    "A = pd.read_pickle('adjacency_small.pkl') \n",
    "A = A[sorted(A.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((878, 290), (878, 878))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[X==1] = 10 #10 #valid\n",
    "X[X==0] = 1 #not valid\n",
    "#X[X==-100] = 0 #missing\n",
    "X = X.astype(float)\n",
    "X[X==-100] = np.nan\n",
    "#np.unique(X)\n",
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = X.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find index of student that answered to most questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425.0\n"
     ]
    }
   ],
   "source": [
    "for col in X:\n",
    "    if (len(X[col][X[col]!=0])> 500):\n",
    "        print(col)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X[123482]!=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([80]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X.columns==123482.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output : flattened input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = X.values.ravel(order = 'F') #len(y) =n*m : 878*290 #rempli en colonne de sorte que \n",
    "                                        #per user_id, the responses to all the exercise_id (valid/not valid/missing)\n",
    "#np.where(y!=0)[0].shape #8821 non null entries (1 or 10)\n",
    "#y[y==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for col in X : \n",
    "    a = pd.get_dummies(X[col])\n",
    "    if (1 not in a.columns) & (10 in a.columns):\n",
    "        a[1] = np.zeros(a.shape[0],dtype=int)\n",
    "    if (1 in a.columns) & (10 not in a.columns):\n",
    "        a[10] = np.zeros(a.shape[0],dtype=int)\n",
    "    l.append(a[[1,10]])\n",
    "X = pd.concat(l, axis=1, keys=X.columns)\n",
    "X.columns = [str(int(col[0]))+'_'+str(col[1]) for col in X.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = scipy.sparse.csr_matrix(A.values)\n",
    "X = np.asmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 580) (878, 290) (878, 878) (254620,)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> <class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csr.csr_matrix'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,X1.shape, A.shape, y.shape) #n*2m, n*m\n",
    "print(type(X),type(X1), type(A), type(y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILTER = 'localpool'  \n",
    "MAX_DEGREE = 2 \n",
    "SYM_NORM = True  \n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  \n",
    "\n",
    "# Normalize X\n",
    "X = X/X.sum(1).reshape(-1, 1)\n",
    "\n",
    "A_ = preprocess_adj(A, SYM_NORM)\n",
    "support = 1\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = pd.get_dummies(y).as_matrix()\n",
    "\n",
    "idx_train = range(70240,71118)\n",
    "idx_val = range(70240,71118)\n",
    "idx_test = range(70240,71118)\n",
    "\n",
    "y_train = y_[idx_train]\n",
    "y_test = y_[idx_test]\n",
    "y_val = y_[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_1 = [X1, A_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in_1 = Input(shape=(X1.shape[1],))\n",
    "H_1 = Dropout(0.5)(X_in_1)\n",
    "H_1 = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H_1]+G)\n",
    "H_1 = Dropout(0.5)(H_1)\n",
    "Y = GraphConvolution(y_.shape[1], support, activation='softmax')([H_1]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_(y_true, y_pred):\n",
    "    list_ind = np.where(y_train!=0)[0]\n",
    "    mask = np.zeros((y_train.shape[0],1))\n",
    "    mask[list_ind] = 1\n",
    "    #mask = keras.utils.to_categorical(mask)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "878/878 [==============================] - 0s 439us/step - loss: nan\n",
      "Epoch 2/100\n",
      "878/878 [==============================] - 0s 169us/step - loss: nan\n",
      "Epoch 3/100\n",
      "878/878 [==============================] - 0s 162us/step - loss: nan\n",
      "Epoch 4/100\n",
      "878/878 [==============================] - 0s 162us/step - loss: nan\n",
      "Epoch 5/100\n",
      "878/878 [==============================] - 0s 171us/step - loss: nan\n",
      "Epoch 6/100\n",
      "878/878 [==============================] - 0s 164us/step - loss: nan\n",
      "Epoch 7/100\n",
      "878/878 [==============================] - 0s 169us/step - loss: nan\n",
      "Epoch 8/100\n",
      "878/878 [==============================] - 0s 166us/step - loss: nan\n",
      "Epoch 9/100\n",
      "878/878 [==============================] - 0s 185us/step - loss: nan\n",
      "Epoch 10/100\n",
      "878/878 [==============================] - 0s 195us/step - loss: nan\n",
      "Epoch 11/100\n",
      "878/878 [==============================] - 0s 250us/step - loss: nan\n",
      "Epoch 12/100\n",
      "878/878 [==============================] - 0s 183us/step - loss: nan\n",
      "Epoch 13/100\n",
      "878/878 [==============================] - 0s 206us/step - loss: nan\n",
      "Epoch 14/100\n",
      "878/878 [==============================] - 0s 200us/step - loss: nan\n",
      "Epoch 15/100\n",
      "878/878 [==============================] - 0s 214us/step - loss: nan\n",
      "Epoch 16/100\n",
      "878/878 [==============================] - 0s 193us/step - loss: nan\n",
      "Epoch 17/100\n",
      "878/878 [==============================] - 0s 241us/step - loss: nan\n",
      "Epoch 18/100\n",
      "878/878 [==============================] - 0s 236us/step - loss: nan\n",
      "Epoch 19/100\n",
      "878/878 [==============================] - 0s 188us/step - loss: nan\n",
      "Epoch 20/100\n",
      "878/878 [==============================] - 0s 193us/step - loss: nan\n",
      "Epoch 21/100\n",
      "878/878 [==============================] - 0s 196us/step - loss: nan\n",
      "Epoch 22/100\n",
      "878/878 [==============================] - 0s 219us/step - loss: nan\n",
      "Epoch 23/100\n",
      "878/878 [==============================] - 0s 200us/step - loss: nan\n",
      "Epoch 24/100\n",
      "878/878 [==============================] - 0s 201us/step - loss: nan\n",
      "Epoch 25/100\n",
      "878/878 [==============================] - 0s 189us/step - loss: nan\n",
      "Epoch 26/100\n",
      "878/878 [==============================] - 0s 172us/step - loss: nan\n",
      "Epoch 27/100\n",
      "878/878 [==============================] - 0s 201us/step - loss: nan\n",
      "Epoch 28/100\n",
      "878/878 [==============================] - 0s 228us/step - loss: nan\n",
      "Epoch 29/100\n",
      "878/878 [==============================] - 0s 183us/step - loss: nan\n",
      "Epoch 30/100\n",
      "878/878 [==============================] - 0s 176us/step - loss: nan\n",
      "Epoch 31/100\n",
      "878/878 [==============================] - 0s 176us/step - loss: nan\n",
      "Epoch 32/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: nan\n",
      "Epoch 33/100\n",
      "878/878 [==============================] - 0s 174us/step - loss: nan\n",
      "Epoch 34/100\n",
      "878/878 [==============================] - 0s 187us/step - loss: nan\n",
      "Epoch 35/100\n",
      "878/878 [==============================] - 0s 198us/step - loss: nan\n",
      "Epoch 36/100\n",
      "878/878 [==============================] - 0s 204us/step - loss: nan\n",
      "Epoch 37/100\n",
      "878/878 [==============================] - 0s 173us/step - loss: nan\n",
      "Epoch 38/100\n",
      "878/878 [==============================] - 0s 243us/step - loss: nan\n",
      "Epoch 39/100\n",
      "878/878 [==============================] - 0s 248us/step - loss: nan\n",
      "Epoch 40/100\n",
      "878/878 [==============================] - 0s 232us/step - loss: nan\n",
      "Epoch 41/100\n",
      "878/878 [==============================] - 0s 198us/step - loss: nan\n",
      "Epoch 42/100\n",
      "878/878 [==============================] - 0s 178us/step - loss: nan\n",
      "Epoch 43/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: nan\n",
      "Epoch 44/100\n",
      "878/878 [==============================] - 0s 184us/step - loss: nan\n",
      "Epoch 45/100\n",
      "878/878 [==============================] - 0s 184us/step - loss: nan\n",
      "Epoch 46/100\n",
      "878/878 [==============================] - 0s 179us/step - loss: nan\n",
      "Epoch 47/100\n",
      "878/878 [==============================] - 0s 189us/step - loss: nan\n",
      "Epoch 48/100\n",
      "878/878 [==============================] - 0s 184us/step - loss: nan\n",
      "Epoch 49/100\n",
      "878/878 [==============================] - 0s 183us/step - loss: nan\n",
      "Epoch 50/100\n",
      "878/878 [==============================] - 0s 181us/step - loss: nan\n",
      "Epoch 51/100\n",
      "878/878 [==============================] - 0s 213us/step - loss: nan\n",
      "Epoch 52/100\n",
      "878/878 [==============================] - 0s 188us/step - loss: nan\n",
      "Epoch 53/100\n",
      "878/878 [==============================] - 0s 187us/step - loss: nan\n",
      "Epoch 54/100\n",
      "878/878 [==============================] - 0s 199us/step - loss: nan\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-92043b8d7b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m h = model.fit(graph_1, y_train, sample_weight=train_mask,\n\u001b[0;32m---> 11\u001b[0;31m           batch_size=A.shape[0], epochs=100, shuffle=False, verbose=1)\n\u001b[0m",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = Model(inputs=[X_in_1]+G, outputs=Y)\n",
    "model.compile(loss=loss_, optimizer=Adam(lr=0.01))\n",
    "\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "train_mask = sample_mask(np.where(y_train!=0)[0], y_train.shape[0])\n",
    "h = model.fit(graph_1, y_train, sample_weight=train_mask,\n",
    "          batch_size=A.shape[0], epochs=100, shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = [X, A_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y_.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "878/878 [==============================] - 1s 622us/step - loss: 0.7088\n",
      "Epoch 2/100\n",
      "878/878 [==============================] - 0s 206us/step - loss: 0.7024\n",
      "Epoch 3/100\n",
      "878/878 [==============================] - 0s 215us/step - loss: 0.6975\n",
      "Epoch 4/100\n",
      "878/878 [==============================] - 0s 240us/step - loss: 0.6936\n",
      "Epoch 5/100\n",
      "878/878 [==============================] - 0s 213us/step - loss: 0.6893\n",
      "Epoch 6/100\n",
      "878/878 [==============================] - 0s 206us/step - loss: 0.6858\n",
      "Epoch 7/100\n",
      "878/878 [==============================] - 0s 210us/step - loss: 0.6825\n",
      "Epoch 8/100\n",
      "878/878 [==============================] - 0s 221us/step - loss: 0.6799\n",
      "Epoch 9/100\n",
      "878/878 [==============================] - 0s 203us/step - loss: 0.6775\n",
      "Epoch 10/100\n",
      "878/878 [==============================] - 0s 194us/step - loss: 0.6743\n",
      "Epoch 11/100\n",
      "878/878 [==============================] - 0s 196us/step - loss: 0.6731\n",
      "Epoch 12/100\n",
      "878/878 [==============================] - 0s 198us/step - loss: 0.6706\n",
      "Epoch 13/100\n",
      "878/878 [==============================] - 0s 366us/step - loss: 0.6686\n",
      "Epoch 14/100\n",
      "878/878 [==============================] - 0s 282us/step - loss: 0.6664\n",
      "Epoch 15/100\n",
      "878/878 [==============================] - 0s 186us/step - loss: 0.6657\n",
      "Epoch 16/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6641\n",
      "Epoch 17/100\n",
      "878/878 [==============================] - 0s 317us/step - loss: 0.6630\n",
      "Epoch 18/100\n",
      "878/878 [==============================] - 0s 191us/step - loss: 0.6625\n",
      "Epoch 19/100\n",
      "878/878 [==============================] - 0s 181us/step - loss: 0.6620\n",
      "Epoch 20/100\n",
      "878/878 [==============================] - 0s 208us/step - loss: 0.6617\n",
      "Epoch 21/100\n",
      "878/878 [==============================] - 0s 285us/step - loss: 0.6614\n",
      "Epoch 22/100\n",
      "878/878 [==============================] - 0s 268us/step - loss: 0.6614\n",
      "Epoch 23/100\n",
      "878/878 [==============================] - 0s 471us/step - loss: 0.6610\n",
      "Epoch 24/100\n",
      "878/878 [==============================] - 0s 433us/step - loss: 0.6607\n",
      "Epoch 25/100\n",
      "878/878 [==============================] - 0s 456us/step - loss: 0.6609\n",
      "Epoch 26/100\n",
      "878/878 [==============================] - 0s 356us/step - loss: 0.6603\n",
      "Epoch 27/100\n",
      "878/878 [==============================] - 0s 342us/step - loss: 0.6598\n",
      "Epoch 28/100\n",
      "878/878 [==============================] - 0s 412us/step - loss: 0.6594\n",
      "Epoch 29/100\n",
      "878/878 [==============================] - 0s 371us/step - loss: 0.6593\n",
      "Epoch 30/100\n",
      "878/878 [==============================] - 0s 441us/step - loss: 0.6587\n",
      "Epoch 31/100\n",
      "878/878 [==============================] - 0s 355us/step - loss: 0.6584\n",
      "Epoch 32/100\n",
      "878/878 [==============================] - 0s 415us/step - loss: 0.6584\n",
      "Epoch 33/100\n",
      "878/878 [==============================] - 0s 187us/step - loss: 0.6585\n",
      "Epoch 34/100\n",
      "878/878 [==============================] - 0s 362us/step - loss: 0.6583\n",
      "Epoch 35/100\n",
      "878/878 [==============================] - 0s 328us/step - loss: 0.6580\n",
      "Epoch 36/100\n",
      "878/878 [==============================] - 0s 330us/step - loss: 0.6575\n",
      "Epoch 37/100\n",
      "878/878 [==============================] - 0s 312us/step - loss: 0.6578\n",
      "Epoch 38/100\n",
      "878/878 [==============================] - 0s 392us/step - loss: 0.6577\n",
      "Epoch 39/100\n",
      "878/878 [==============================] - 0s 290us/step - loss: 0.6575\n",
      "Epoch 40/100\n",
      "878/878 [==============================] - 0s 181us/step - loss: 0.6577\n",
      "Epoch 41/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6572\n",
      "Epoch 42/100\n",
      "878/878 [==============================] - 0s 264us/step - loss: 0.6571\n",
      "Epoch 43/100\n",
      "878/878 [==============================] - 0s 237us/step - loss: 0.6570\n",
      "Epoch 44/100\n",
      "878/878 [==============================] - 0s 182us/step - loss: 0.6570\n",
      "Epoch 45/100\n",
      "878/878 [==============================] - 0s 187us/step - loss: 0.6569\n",
      "Epoch 46/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6570\n",
      "Epoch 47/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6567\n",
      "Epoch 48/100\n",
      "878/878 [==============================] - 0s 179us/step - loss: 0.6567\n",
      "Epoch 49/100\n",
      "878/878 [==============================] - 0s 185us/step - loss: 0.6565\n",
      "Epoch 50/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6564\n",
      "Epoch 51/100\n",
      "878/878 [==============================] - 0s 182us/step - loss: 0.6565\n",
      "Epoch 52/100\n",
      "878/878 [==============================] - 0s 189us/step - loss: 0.6564\n",
      "Epoch 53/100\n",
      "878/878 [==============================] - 0s 180us/step - loss: 0.6562\n",
      "Epoch 54/100\n",
      "878/878 [==============================] - 0s 186us/step - loss: 0.6563\n",
      "Epoch 55/100\n",
      "878/878 [==============================] - 0s 217us/step - loss: 0.6562\n",
      "Epoch 56/100\n",
      "878/878 [==============================] - 0s 235us/step - loss: 0.6564\n",
      "Epoch 57/100\n",
      "878/878 [==============================] - 0s 219us/step - loss: 0.6561\n",
      "Epoch 58/100\n",
      "878/878 [==============================] - 0s 227us/step - loss: 0.6561\n",
      "Epoch 59/100\n",
      "878/878 [==============================] - 0s 222us/step - loss: 0.6562\n",
      "Epoch 60/100\n",
      "878/878 [==============================] - 0s 263us/step - loss: 0.6562\n",
      "Epoch 61/100\n",
      "878/878 [==============================] - 0s 191us/step - loss: 0.6559\n",
      "Epoch 62/100\n",
      "878/878 [==============================] - 0s 188us/step - loss: 0.6559\n",
      "Epoch 63/100\n",
      "878/878 [==============================] - 0s 199us/step - loss: 0.6562\n",
      "Epoch 64/100\n",
      "878/878 [==============================] - 0s 195us/step - loss: 0.6559\n",
      "Epoch 65/100\n",
      "878/878 [==============================] - 0s 234us/step - loss: 0.6561\n",
      "Epoch 66/100\n",
      "878/878 [==============================] - 0s 188us/step - loss: 0.6558\n",
      "Epoch 67/100\n",
      "878/878 [==============================] - 0s 184us/step - loss: 0.6560\n",
      "Epoch 68/100\n",
      "878/878 [==============================] - 0s 249us/step - loss: 0.6560\n",
      "Epoch 69/100\n",
      "878/878 [==============================] - 1s 673us/step - loss: 0.6557\n",
      "Epoch 70/100\n",
      "878/878 [==============================] - 0s 215us/step - loss: 0.6558\n",
      "Epoch 71/100\n",
      "878/878 [==============================] - 0s 231us/step - loss: 0.6558\n",
      "Epoch 72/100\n",
      "878/878 [==============================] - 0s 223us/step - loss: 0.6556\n",
      "Epoch 73/100\n",
      "878/878 [==============================] - 0s 190us/step - loss: 0.6556\n",
      "Epoch 74/100\n",
      "878/878 [==============================] - 0s 233us/step - loss: 0.6556\n",
      "Epoch 75/100\n",
      "878/878 [==============================] - 0s 189us/step - loss: 0.6558\n",
      "Epoch 76/100\n",
      "878/878 [==============================] - 0s 192us/step - loss: 0.6555\n",
      "Epoch 77/100\n",
      "878/878 [==============================] - 0s 202us/step - loss: 0.6554\n",
      "Epoch 78/100\n",
      "878/878 [==============================] - 0s 360us/step - loss: 0.6556\n",
      "Epoch 79/100\n",
      "878/878 [==============================] - 0s 390us/step - loss: 0.6557\n",
      "Epoch 80/100\n",
      "878/878 [==============================] - 0s 329us/step - loss: 0.6556\n",
      "Epoch 81/100\n",
      "878/878 [==============================] - 0s 317us/step - loss: 0.6555\n",
      "Epoch 82/100\n",
      "878/878 [==============================] - 0s 328us/step - loss: 0.6556\n",
      "Epoch 83/100\n",
      "878/878 [==============================] - 0s 339us/step - loss: 0.6556\n",
      "Epoch 84/100\n",
      "878/878 [==============================] - 0s 314us/step - loss: 0.6554\n",
      "Epoch 85/100\n",
      "878/878 [==============================] - 0s 458us/step - loss: 0.6554\n",
      "Epoch 86/100\n",
      "878/878 [==============================] - 0s 435us/step - loss: 0.6555\n",
      "Epoch 87/100\n",
      "878/878 [==============================] - 0s 354us/step - loss: 0.6556\n",
      "Epoch 88/100\n",
      "878/878 [==============================] - 0s 455us/step - loss: 0.6554\n",
      "Epoch 89/100\n",
      "878/878 [==============================] - 0s 370us/step - loss: 0.6555\n",
      "Epoch 90/100\n",
      "878/878 [==============================] - 0s 306us/step - loss: 0.6554\n",
      "Epoch 91/100\n",
      "878/878 [==============================] - 0s 214us/step - loss: 0.6555\n",
      "Epoch 92/100\n",
      "878/878 [==============================] - 0s 311us/step - loss: 0.6555\n",
      "Epoch 93/100\n",
      "878/878 [==============================] - 0s 322us/step - loss: 0.6554\n",
      "Epoch 94/100\n",
      "878/878 [==============================] - 0s 377us/step - loss: 0.6556\n",
      "Epoch 95/100\n",
      "878/878 [==============================] - 0s 209us/step - loss: 0.6553\n",
      "Epoch 96/100\n",
      "878/878 [==============================] - 0s 442us/step - loss: 0.6555\n",
      "Epoch 97/100\n",
      "878/878 [==============================] - 0s 344us/step - loss: 0.6558\n",
      "Epoch 98/100\n",
      "878/878 [==============================] - 0s 357us/step - loss: 0.6554\n",
      "Epoch 99/100\n",
      "878/878 [==============================] - 0s 369us/step - loss: 0.6553\n",
      "Epoch 100/100\n",
      "878/878 [==============================] - 0s 324us/step - loss: 0.6552\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "train_mask = sample_mask(np.where(y_train!=0)[0], y_train.shape[0])\n",
    "h = model.fit(graph, y_train, sample_weight=train_mask,\n",
    "          batch_size=A.shape[0], epochs=100, shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDLEoW9mliH/H+y66a4FddN+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12cd7c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#h['loss']\n",
    "plt.plot(range(100), h.history['loss'])\n",
    "plt.ylim([0.6, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def accuracy(preds, labels):\n",
    "#    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "#def evaluate_preds(preds, labels, indices):\n",
    "\n",
    "#    split_loss = list()\n",
    "#    split_acc = list()\n",
    "\n",
    "#    for y_split, idx_split in zip(labels, indices):\n",
    "#        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "#        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "\n",
    "#    return split_loss, split_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "#for epoch in range(1, NB_EPOCH+1):\n",
    "#    t = time.time()\n",
    "#    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "#              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=1)\n",
    "#    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "    \n",
    "    # Train / validation scores\n",
    "    #train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "    #                                               [idx_train, idx_val])\n",
    "    #print(\"Epoch: {:04d}\".format(epoch),\n",
    "    #      \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "    #      \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "    #      \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "    #      \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "    #      \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    #if train_val_loss[1] < best_val_loss:\n",
    "    #    best_val_loss = train_val_loss[1]\n",
    "    #    wait = 0\n",
    "    #else:\n",
    "    #    if wait >= PATIENCE:\n",
    "    #        print('Epoch {}: early stopping'.format(epoch))\n",
    "    #        break\n",
    "    #    wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63996005,  0.63444799,  0.63520533,  0.64009714,  0.63234168,\n",
       "        0.63747787,  0.63825715,  0.63770497,  0.64345545,  0.63573956,\n",
       "        0.64391965,  0.64381993,  0.63777018,  0.64152914,  0.63753581,\n",
       "        0.64364851,  0.64399827,  0.64256948,  0.64305782,  0.63900357,\n",
       "        0.64046633,  0.63822848,  0.63812447,  0.64194053,  0.64130247,\n",
       "        0.64543968,  0.64205146,  0.64393175,  0.64007658,  0.63747674,\n",
       "        0.63492054,  0.63923603,  0.63794911,  0.63619268,  0.64514226,\n",
       "        0.63911515,  0.64275992,  0.63656229,  0.62794489,  0.644786  ,\n",
       "        0.64489186,  0.64151686,  0.6425736 ,  0.64105469,  0.63920271,\n",
       "        0.64203387,  0.64175683,  0.64177012,  0.64199787,  0.64499164,\n",
       "        0.64530486,  0.64223486,  0.64069086,  0.63611156,  0.6429525 ,\n",
       "        0.63480586,  0.64016277,  0.64062309,  0.63763011,  0.6409412 ,\n",
       "        0.6449827 ,  0.64520359,  0.63978857,  0.64492613,  0.64120322,\n",
       "        0.64479399,  0.64503801,  0.6391592 ,  0.64515334,  0.64097494,\n",
       "        0.64306235,  0.64226651,  0.64212924,  0.63373804,  0.6340673 ,\n",
       "        0.63290459,  0.63278365,  0.6342445 ,  0.63395643,  0.63725424,\n",
       "        0.63283873,  0.63284212,  0.63415921,  0.63276273,  0.64356148,\n",
       "        0.63656998,  0.63890624,  0.64200246,  0.64366484,  0.64498651,\n",
       "        0.64120668,  0.64316088,  0.64542145,  0.64172339,  0.63987875,\n",
       "        0.64014876,  0.6420238 ,  0.64197588,  0.64077097,  0.64529717,\n",
       "        0.6396569 ,  0.6390304 ,  0.64370823,  0.64193434,  0.64246076,\n",
       "        0.64411408,  0.64444935,  0.64126605,  0.64408427,  0.64117396,\n",
       "        0.63882571,  0.63880742,  0.64420635,  0.6402446 ,  0.64089519,\n",
       "        0.64192748,  0.64381844,  0.64540839,  0.64186817,  0.64276081,\n",
       "        0.63922703,  0.63457668,  0.64030617,  0.64388978,  0.63924968,\n",
       "        0.63891292,  0.6446209 ,  0.63554907,  0.63841826,  0.64430159,\n",
       "        0.64086211,  0.64291906,  0.64322084,  0.64358276,  0.6432296 ,\n",
       "        0.63978016,  0.634606  ,  0.64323205,  0.63790679,  0.63244218,\n",
       "        0.64098686,  0.64114434,  0.64229971,  0.63391179,  0.63757503,\n",
       "        0.64059782,  0.63995963,  0.63907588,  0.64016813,  0.6333999 ,\n",
       "        0.64382833,  0.63382292,  0.64057505,  0.6370402 ,  0.63558072,\n",
       "        0.63483161,  0.6346485 ,  0.63273162,  0.63023019,  0.63617802,\n",
       "        0.63345236,  0.63758355,  0.6364159 ,  0.63635612,  0.63643181,\n",
       "        0.63411105,  0.63420242,  0.63536203,  0.63542509,  0.63884699,\n",
       "        0.63648611,  0.63565296,  0.64411902,  0.64079565,  0.64078826,\n",
       "        0.63901967,  0.63637793,  0.63861793,  0.63991886,  0.64352369,\n",
       "        0.63578761,  0.63631839,  0.63831657,  0.63512248,  0.64051998,\n",
       "        0.6381405 ,  0.63444561,  0.63934988,  0.63790745,  0.64216167,\n",
       "        0.64094347,  0.6433937 ,  0.63673109,  0.63574237,  0.63611889,\n",
       "        0.63784623,  0.63632709,  0.63643301,  0.64160168,  0.64500856,\n",
       "        0.64515555,  0.64197296,  0.64184242,  0.64221114,  0.63476455,\n",
       "        0.63322091,  0.63989121,  0.64462286,  0.64072663,  0.6412797 ,\n",
       "        0.63685971,  0.63881832,  0.63993001,  0.64165127,  0.63693255,\n",
       "        0.64168614,  0.63750845,  0.63829058,  0.64339721,  0.6444546 ,\n",
       "        0.64283741,  0.64476448,  0.6293503 ,  0.64222234,  0.63079876,\n",
       "        0.64221811,  0.63743114,  0.63539034,  0.64116943,  0.64375401,\n",
       "        0.64281684,  0.64094776,  0.64271402,  0.63901263,  0.64171618,\n",
       "        0.64207441,  0.64233899,  0.64453822,  0.64006144,  0.64502764,\n",
       "        0.64503074,  0.64416265,  0.64474303,  0.64079517,  0.63746053,\n",
       "        0.6411323 ,  0.63957709,  0.64456058,  0.64200079,  0.64181423,\n",
       "        0.64053482,  0.63909191,  0.64125353,  0.63570875,  0.63940513,\n",
       "        0.64124852,  0.63853872,  0.63919699,  0.63672054,  0.64434248,\n",
       "        0.63976777,  0.64176267,  0.63370007,  0.64203781,  0.64258188,\n",
       "        0.64166164,  0.63661128,  0.64476478,  0.64093733,  0.64108175,\n",
       "        0.64261305,  0.63916147,  0.64075726,  0.64067161,  0.63780832,\n",
       "        0.63825399,  0.64522731,  0.63811368,  0.64161378,  0.64168394,\n",
       "        0.63755047,  0.64390969,  0.64456344,  0.63816136,  0.6421898 ,\n",
       "        0.64025301,  0.64079642,  0.64340037,  0.64185607,  0.64164931,\n",
       "        0.63912433,  0.64102036,  0.63932478,  0.64390445,  0.64319283,\n",
       "        0.63884085,  0.64186597,  0.64299262,  0.63073587,  0.6428867 ,\n",
       "        0.64019579,  0.63849372,  0.63889068,  0.64101648,  0.64096493,\n",
       "        0.6424039 ,  0.64319903,  0.64186388,  0.63703656,  0.63774544,\n",
       "        0.63321751,  0.64012891,  0.63279235,  0.63307071,  0.63815916,\n",
       "        0.63816619,  0.63128257,  0.63494265,  0.63781917,  0.63409644,\n",
       "        0.63311589,  0.63120908,  0.63442993,  0.6380477 ,  0.63211459,\n",
       "        0.63317257,  0.63540906,  0.6396969 ,  0.64152116,  0.6423046 ,\n",
       "        0.64231747,  0.63836175,  0.64377189,  0.63958871,  0.64482069,\n",
       "        0.63901848,  0.64172143,  0.64226282,  0.63843584,  0.64445043,\n",
       "        0.64069492,  0.64467084,  0.6451624 ,  0.63937032,  0.63377559,\n",
       "        0.64357728,  0.64526016,  0.64526784,  0.63739628,  0.64432162,\n",
       "        0.63612622,  0.6404264 ,  0.64525265,  0.63482386,  0.64064908,\n",
       "        0.636531  ,  0.64404207,  0.6450839 ,  0.64462793,  0.64256847,\n",
       "        0.64023465,  0.64230537,  0.64221829,  0.64164984,  0.64508706,\n",
       "        0.64220661,  0.63280332,  0.63881433,  0.63728732,  0.64173162,\n",
       "        0.63317943,  0.64159423,  0.6348151 ,  0.64170176,  0.6414116 ,\n",
       "        0.63924211,  0.64171129,  0.64437711,  0.63972521,  0.64228684,\n",
       "        0.64083302,  0.64183462,  0.63975155,  0.64247191,  0.64371789,\n",
       "        0.64285558,  0.64418858,  0.64051086,  0.63932788,  0.64502686,\n",
       "        0.63993138,  0.64092588,  0.64216441,  0.64338332,  0.6396805 ,\n",
       "        0.63972074,  0.63760209,  0.63914716,  0.63979119,  0.63706422,\n",
       "        0.6406129 ,  0.64317513,  0.63919514,  0.64393872,  0.64282662,\n",
       "        0.64508563,  0.64304143,  0.63859832,  0.64098978,  0.63958579,\n",
       "        0.64347088,  0.64083165,  0.64040881,  0.63867801,  0.63798636,\n",
       "        0.64504933,  0.64227915,  0.64090508,  0.64160055,  0.62714672,\n",
       "        0.64381659,  0.64306009,  0.64172691,  0.64186281,  0.64085257,\n",
       "        0.6319465 ,  0.6441918 ,  0.64098048,  0.64387214,  0.64091009,\n",
       "        0.63891739,  0.63419223,  0.63779956,  0.64344597,  0.640522  ,\n",
       "        0.63438135,  0.632456  ,  0.63791996,  0.63838613,  0.63592106,\n",
       "        0.63350964,  0.6368891 ,  0.63337523,  0.63112122,  0.63809919,\n",
       "        0.64049727,  0.64375561,  0.64388686,  0.64379239,  0.63860744,\n",
       "        0.64409733,  0.64124113,  0.6418649 ,  0.64172566,  0.64253736,\n",
       "        0.64414489,  0.63571179,  0.63681895,  0.64177483,  0.63732076,\n",
       "        0.6153605 ,  0.6371538 ,  0.6388396 ,  0.62581789,  0.63360727,\n",
       "        0.6394695 ,  0.64327514,  0.64118135,  0.64241356,  0.63640517,\n",
       "        0.64010012,  0.63884425,  0.63725936,  0.63779747,  0.63625014,\n",
       "        0.63879955,  0.63416654,  0.63945699,  0.63652843,  0.63433599,\n",
       "        0.63086307,  0.63846242,  0.6371088 ,  0.63394856,  0.6331259 ,\n",
       "        0.63380212,  0.64037699,  0.64014971,  0.64462626,  0.64397299,\n",
       "        0.63859355,  0.64374471,  0.64301199,  0.64105463,  0.64352542,\n",
       "        0.63714457,  0.63360417,  0.64332265,  0.63868982,  0.64419848,\n",
       "        0.63990158,  0.64311248,  0.63488871,  0.64073658,  0.63947964,\n",
       "        0.638412  ,  0.64063686,  0.64308149,  0.6444599 ,  0.63988739,\n",
       "        0.64088494,  0.64151222,  0.64022011,  0.63938129,  0.64548194,\n",
       "        0.63728374,  0.64295334,  0.64323771,  0.6415838 ,  0.6365236 ,\n",
       "        0.63997442,  0.64183426,  0.63783425,  0.64243186,  0.64333415,\n",
       "        0.6410861 ,  0.64312536,  0.63993812,  0.63762647,  0.63914502,\n",
       "        0.64113283,  0.64470726,  0.63848019,  0.63857102,  0.64211541,\n",
       "        0.64366025,  0.6419543 ,  0.64399481,  0.64521557,  0.64072847,\n",
       "        0.6385811 ,  0.64015001,  0.63785565,  0.64254475,  0.63569093,\n",
       "        0.63922662,  0.63172507,  0.63681728,  0.63731909,  0.6357367 ,\n",
       "        0.63669235,  0.64221656,  0.63043696,  0.6369406 ,  0.63492388,\n",
       "        0.63345551,  0.63657308,  0.63399643,  0.63744467,  0.63530064,\n",
       "        0.63740551,  0.63842148,  0.64002353,  0.63909227,  0.63854259,\n",
       "        0.64076   ,  0.64041573,  0.64184403,  0.64201719,  0.6247313 ,\n",
       "        0.64372426,  0.63321668,  0.63692254,  0.63717288,  0.63710982,\n",
       "        0.63694394,  0.64310187,  0.63217539,  0.6363247 ,  0.6370548 ,\n",
       "        0.6367299 ,  0.63683307,  0.64384645,  0.64250761,  0.6437304 ,\n",
       "        0.64206326,  0.64182329,  0.63913232,  0.64368737,  0.64344311,\n",
       "        0.64286739,  0.6392526 ,  0.64097607,  0.63997537,  0.63728338,\n",
       "        0.63908964,  0.64293104,  0.63676894,  0.64152473,  0.63674313,\n",
       "        0.62612963,  0.63617349,  0.63403648,  0.62855828,  0.63758796,\n",
       "        0.63647574,  0.6383903 ,  0.63349921,  0.6359458 ,  0.63092047,\n",
       "        0.63646138,  0.63425285,  0.63811421,  0.63796163,  0.64031929,\n",
       "        0.63551533,  0.63692111,  0.62662858,  0.6374926 ,  0.64124852,\n",
       "        0.64095169,  0.64320248,  0.63701773,  0.64197767,  0.64475888,\n",
       "        0.6404196 ,  0.64369279,  0.64291251,  0.64492631,  0.6384328 ,\n",
       "        0.63810802,  0.64465344,  0.6381442 ,  0.63263541,  0.64514256,\n",
       "        0.63997358,  0.63903803,  0.64143455,  0.64450431,  0.64148921,\n",
       "        0.64513332,  0.63920468,  0.64116383,  0.63388073,  0.64047468,\n",
       "        0.63851792,  0.63906109,  0.64184624,  0.6419692 ,  0.63734138,\n",
       "        0.64471352,  0.64216572,  0.6417647 ,  0.64437538,  0.64045984,\n",
       "        0.64286977,  0.64205974,  0.63787478,  0.64131498,  0.63832915,\n",
       "        0.63153124,  0.63619137,  0.64490521,  0.64455318,  0.64076048,\n",
       "        0.64010453,  0.63199323,  0.62258404,  0.62936527,  0.6299513 ,\n",
       "        0.62214988,  0.6235804 ,  0.63232416,  0.63418412,  0.62803453,\n",
       "        0.63342363,  0.63501042,  0.61621469,  0.6334976 ,  0.6242938 ,\n",
       "        0.63840669,  0.63027251,  0.62331533,  0.64061564,  0.63494313,\n",
       "        0.63312316,  0.61300421,  0.6326279 ,  0.62924522,  0.6401642 ,\n",
       "        0.63106865,  0.63268083,  0.64179665,  0.63606167,  0.63313377,\n",
       "        0.64058536,  0.64107257,  0.62613589,  0.64016896,  0.63508928,\n",
       "        0.64271826,  0.6412487 ,  0.64237607,  0.64172149,  0.63037497,\n",
       "        0.63386643,  0.62744635,  0.63137013,  0.63429958,  0.61712825,\n",
       "        0.63179904,  0.63284773,  0.63517797,  0.63180542,  0.63758725,\n",
       "        0.64154142,  0.63063508,  0.64305091,  0.63883096,  0.64010257,\n",
       "        0.61531329,  0.6403814 ,  0.63877332,  0.63978189,  0.63284618,\n",
       "        0.63786435,  0.63934636,  0.63539362,  0.6418907 ,  0.63402313,\n",
       "        0.6429075 ,  0.6450873 ,  0.64179075,  0.63262761,  0.63383055,\n",
       "        0.63959742,  0.63528931,  0.64148879,  0.64364392,  0.64373434,\n",
       "        0.633295  ,  0.63642067,  0.64297372,  0.63840699,  0.63593107,\n",
       "        0.64341885,  0.63986033,  0.63488799,  0.6398077 ,  0.64403808,\n",
       "        0.64391375,  0.64206493,  0.63974249,  0.64036977,  0.64307922,\n",
       "        0.64250439,  0.64317501,  0.63278824,  0.63991839,  0.64205456,\n",
       "        0.61649984,  0.63932282,  0.63898617,  0.63704628,  0.6372816 ,\n",
       "        0.64399165,  0.61649346,  0.63022333,  0.64063257,  0.63669443,\n",
       "        0.64331722,  0.64411277,  0.64179838,  0.64393348,  0.63666278,\n",
       "        0.64450061,  0.64492172,  0.64487249,  0.64523244,  0.64299035,\n",
       "        0.64198178,  0.63263327,  0.63976848,  0.64225465,  0.64030415,\n",
       "        0.63621348,  0.64095503,  0.64152098,  0.64003325,  0.63729018,\n",
       "        0.64539307,  0.64220089,  0.64399344,  0.63938886,  0.64208305,\n",
       "        0.64197218,  0.6426537 ,  0.64429313,  0.63994247,  0.6440829 ,\n",
       "        0.61649346,  0.64161915,  0.6390847 ,  0.63358545,  0.64533311,\n",
       "        0.63821661,  0.61649346,  0.64267105,  0.64465404,  0.64041024,\n",
       "        0.63754326,  0.63725501,  0.64369583,  0.64521706,  0.6377278 ,\n",
       "        0.64428127,  0.64199489,  0.64045054,  0.6393221 ,  0.64426941,\n",
       "        0.64216352,  0.63987553,  0.63665283,  0.63850307,  0.63319981,\n",
       "        0.64434761,  0.64539641,  0.64219439,  0.64189339,  0.63293737,\n",
       "        0.63062626,  0.64123899,  0.64117086,  0.64127922,  0.63976872,\n",
       "        0.64237821,  0.64135695,  0.64299977,  0.6399138 ,  0.6429826 ,\n",
       "        0.64039302,  0.64346868,  0.63489276,  0.64418232,  0.63478369,\n",
       "        0.63411212,  0.62751549,  0.63398796,  0.63001317,  0.61931539,\n",
       "        0.63478053,  0.62855619,  0.60723245,  0.63190788,  0.63923079,\n",
       "        0.63683712,  0.64086986,  0.63632166,  0.63882893,  0.64217412,\n",
       "        0.63632756,  0.63626128,  0.6363706 ,  0.63371027,  0.63695157,\n",
       "        0.63806283,  0.63901925,  0.63679922,  0.63933063,  0.63996375,\n",
       "        0.63958299,  0.63738173,  0.63798887,  0.62826586,  0.62583095,\n",
       "        0.63501835,  0.63716394,  0.6425761 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6414 accuracy= 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
