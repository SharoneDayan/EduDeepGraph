{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:47:18.850263Z",
     "start_time": "2017-12-28T18:47:18.838281Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "# Import T. Kipf's GCN implementation\n",
    "# https://github.com/tkipf/keras-gcn\n",
    "sys.path.append('./keras-gcn/')\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_hdf('history_small.hdf', key='hist') \n",
    "A = pd.read_pickle('adjacency_small.pkl') \n",
    "A = A[sorted(A.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One student and all the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "student = 142954.0\n",
    "X_k_ = X[student]\n",
    "X_k, ex_k = X_k_[X_k_!=-100], X_k_[X_k_!=-100].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y = pd.get_dummies(X_k_).as_matrix() #-100 0 1\n",
    "y = pd.get_dummies(X_k_)[[0,1]].as_matrix() #0 1\n",
    "A = scipy.sparse.csr_matrix(A.values)\n",
    "X = np.asmatrix(X_k_.to_frame().as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 878) (878, 1) (878, 2)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> <class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(A.shape, X.shape, y.shape)\n",
    "print(type(A), type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([327, 328, 330, 333, 338, 340, 347, 349, 352, 355, 358, 364, 371,\n",
       "       372, 380, 381, 385, 387, 436, 442])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind, _ = np.where(X!=-100)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_splits(y):\n",
    "    idx_train = sorted(random.sample(list(ind), 15))\n",
    "    idx_val = sorted(random.sample([k for k in ind if k not in idx_train],5))\n",
    "    idx_test = sorted(random.sample([k for k in range(878) if k not in ind],100))\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize X ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_ = preprocess_adj(A, SYM_NORM)\n",
    "support = 1\n",
    "graph = [X, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 16.2609 train_acc= 0.7333 val_loss= 11.9722 val_acc= 0.8000 time= 0.9538\n",
      "Epoch: 0002 train_loss= 15.7679 train_acc= 0.7333 val_loss= 11.6092 val_acc= 0.8000 time= 0.0674\n",
      "Epoch: 0003 train_loss= 15.2761 train_acc= 0.7333 val_loss= 11.2471 val_acc= 0.8000 time= 0.0872\n",
      "Epoch: 0004 train_loss= 14.7866 train_acc= 0.7333 val_loss= 10.8867 val_acc= 0.8000 time= 0.0880\n",
      "Epoch: 0005 train_loss= 14.3000 train_acc= 0.7333 val_loss= 10.5285 val_acc= 0.8000 time= 0.0843\n",
      "Epoch: 0006 train_loss= 13.9094 train_acc= 0.7333 val_loss= 10.2409 val_acc= 0.8000 time= 0.0894\n",
      "Epoch: 0007 train_loss= 13.4924 train_acc= 0.7333 val_loss= 9.9339 val_acc= 0.8000 time= 0.0901\n",
      "Epoch: 0008 train_loss= 13.0622 train_acc= 0.7333 val_loss= 9.6171 val_acc= 0.8000 time= 0.1007\n",
      "Epoch: 0009 train_loss= 12.6242 train_acc= 0.7333 val_loss= 9.2946 val_acc= 0.8000 time= 0.1129\n",
      "Epoch: 0010 train_loss= 12.1737 train_acc= 0.7333 val_loss= 8.9630 val_acc= 0.8000 time= 0.1134\n",
      "Epoch: 0011 train_loss= 11.7618 train_acc= 0.7333 val_loss= 8.6597 val_acc= 0.8000 time= 0.1112\n",
      "Epoch: 0012 train_loss= 11.3693 train_acc= 0.7333 val_loss= 8.3707 val_acc= 0.8000 time= 0.0871\n",
      "Epoch: 0013 train_loss= 10.9716 train_acc= 0.7333 val_loss= 8.0779 val_acc= 0.8000 time= 0.0899\n",
      "Epoch: 0014 train_loss= 10.5720 train_acc= 0.7333 val_loss= 7.7837 val_acc= 0.8000 time= 0.0924\n",
      "Epoch: 0015 train_loss= 10.1732 train_acc= 0.7333 val_loss= 7.4901 val_acc= 0.8000 time= 0.0885\n",
      "Epoch: 0016 train_loss= 9.7783 train_acc= 0.7333 val_loss= 7.1993 val_acc= 0.8000 time= 0.0884\n",
      "Epoch: 0017 train_loss= 9.3899 train_acc= 0.7333 val_loss= 6.9133 val_acc= 0.8000 time= 0.0951\n",
      "Epoch: 0018 train_loss= 9.0452 train_acc= 0.7333 val_loss= 6.6595 val_acc= 0.8000 time= 0.1016\n",
      "Epoch: 0019 train_loss= 8.6361 train_acc= 0.7333 val_loss= 6.3584 val_acc= 0.8000 time= 0.1230\n",
      "Epoch: 0020 train_loss= 8.2401 train_acc= 0.7333 val_loss= 6.0668 val_acc= 0.8000 time= 0.1339\n",
      "Epoch: 0021 train_loss= 7.8516 train_acc= 0.7333 val_loss= 5.7808 val_acc= 0.8000 time= 0.1004\n",
      "Epoch: 0022 train_loss= 7.4739 train_acc= 0.7333 val_loss= 5.5027 val_acc= 0.8000 time= 0.1132\n",
      "Epoch: 0023 train_loss= 7.1155 train_acc= 0.7333 val_loss= 5.2388 val_acc= 0.8000 time= 0.0841\n",
      "Epoch: 0024 train_loss= 6.7725 train_acc= 0.7333 val_loss= 4.9863 val_acc= 0.8000 time= 0.0903\n",
      "Epoch: 0025 train_loss= 6.4447 train_acc= 0.7333 val_loss= 4.7449 val_acc= 0.8000 time= 0.1048\n",
      "Epoch: 0026 train_loss= 6.1320 train_acc= 0.7333 val_loss= 4.5147 val_acc= 0.8000 time= 0.1104\n",
      "Epoch: 0027 train_loss= 5.8340 train_acc= 0.7333 val_loss= 4.2953 val_acc= 0.8000 time= 0.1153\n",
      "Epoch: 0028 train_loss= 5.5505 train_acc= 0.7333 val_loss= 4.0866 val_acc= 0.8000 time= 0.1188\n",
      "Epoch: 0029 train_loss= 5.2809 train_acc= 0.7333 val_loss= 3.8881 val_acc= 0.8000 time= 0.1269\n",
      "Epoch: 0030 train_loss= 5.0335 train_acc= 0.7333 val_loss= 3.7060 val_acc= 0.8000 time= 0.1805\n",
      "Epoch: 0031 train_loss= 4.8374 train_acc= 0.7333 val_loss= 3.5616 val_acc= 0.8000 time= 0.1126\n",
      "Epoch: 0032 train_loss= 4.6486 train_acc= 0.7333 val_loss= 3.4225 val_acc= 0.8000 time= 0.1000\n",
      "Epoch: 0033 train_loss= 4.4626 train_acc= 0.7333 val_loss= 3.2856 val_acc= 0.8000 time= 0.1145\n",
      "Epoch: 0034 train_loss= 3.6724 train_acc= 0.7333 val_loss= 2.7038 val_acc= 0.8000 time= 0.1098\n",
      "Epoch: 0035 train_loss= 2.7608 train_acc= 0.7333 val_loss= 2.0326 val_acc= 0.8000 time= 0.1284\n",
      "Epoch: 0036 train_loss= 1.7218 train_acc= 0.7333 val_loss= 1.2681 val_acc= 0.8000 time= 0.1410\n",
      "Epoch: 0037 train_loss= 0.6932 train_acc= 0.7333 val_loss= 0.5382 val_acc= 0.8000 time= 0.1383\n",
      "Epoch: 0038 train_loss= 1.6287 train_acc= 0.2667 val_loss= 1.7728 val_acc= 0.2000 time= 0.1180\n",
      "Epoch: 0039 train_loss= 1.9910 train_acc= 0.2667 val_loss= 2.1744 val_acc= 0.2000 time= 0.0942\n",
      "Epoch: 0040 train_loss= 1.3678 train_acc= 0.2667 val_loss= 1.4813 val_acc= 0.2000 time= 0.0976\n",
      "Epoch: 0041 train_loss= 0.6694 train_acc= 0.7333 val_loss= 0.6618 val_acc= 0.8000 time= 0.0946\n",
      "Epoch: 0042 train_loss= 0.6539 train_acc= 0.7333 val_loss= 0.5167 val_acc= 0.8000 time= 0.1018\n",
      "Epoch: 0043 train_loss= 0.9052 train_acc= 0.7333 val_loss= 0.6766 val_acc= 0.8000 time= 0.0954\n",
      "Epoch: 0044 train_loss= 1.0653 train_acc= 0.7333 val_loss= 0.7895 val_acc= 0.8000 time= 0.0966\n",
      "Epoch: 0045 train_loss= 0.9857 train_acc= 0.7333 val_loss= 0.7330 val_acc= 0.8000 time= 0.1077\n",
      "Epoch: 0046 train_loss= 0.7686 train_acc= 0.7333 val_loss= 0.5847 val_acc= 0.8000 time= 0.1016\n",
      "Epoch: 0047 train_loss= 0.5835 train_acc= 0.7333 val_loss= 0.5003 val_acc= 0.8000 time= 0.0955\n",
      "Epoch: 0048 train_loss= 0.7995 train_acc= 0.2667 val_loss= 0.8262 val_acc= 0.2000 time= 0.0899\n",
      "Epoch: 0049 train_loss= 0.8614 train_acc= 0.2667 val_loss= 0.9006 val_acc= 0.2000 time= 0.0922\n",
      "Epoch: 0050 train_loss= 0.6266 train_acc= 0.7333 val_loss= 0.6019 val_acc= 0.8000 time= 0.0974\n",
      "Epoch: 0051 train_loss= 0.6237 train_acc= 0.7333 val_loss= 0.5031 val_acc= 0.8000 time= 0.0968\n",
      "Epoch: 0052 train_loss= 0.7524 train_acc= 0.7333 val_loss= 0.5743 val_acc= 0.8000 time= 0.0895\n",
      "Epoch: 0053 train_loss= 0.7977 train_acc= 0.7333 val_loss= 0.6037 val_acc= 0.8000 time= 0.1014\n",
      "Epoch: 0054 train_loss= 0.7517 train_acc= 0.7333 val_loss= 0.5738 val_acc= 0.8000 time= 0.1130\n",
      "Epoch: 0055 train_loss= 0.6330 train_acc= 0.7333 val_loss= 0.5069 val_acc= 0.8000 time= 0.1246\n",
      "Epoch: 0056 train_loss= 0.5852 train_acc= 0.7333 val_loss= 0.5292 val_acc= 0.8000 time= 0.1202\n",
      "Epoch: 0057 train_loss= 0.6512 train_acc= 0.7333 val_loss= 0.6370 val_acc= 0.8000 time= 0.1293\n",
      "Epoch: 0058 train_loss= 0.6043 train_acc= 0.7333 val_loss= 0.5668 val_acc= 0.8000 time= 0.1193\n",
      "Epoch 58: early stopping\n",
      "Test set results: loss= nan accuracy= 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/sharonedayan/tensorflow/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37097481,\n",
       " 0.37660295,\n",
       " 0.37587154,\n",
       " 0.37080279,\n",
       " 0.37766799,\n",
       " 0.37366334,\n",
       " 0.37290075,\n",
       " 0.37310949,\n",
       " 0.36749214,\n",
       " 0.37451833,\n",
       " 0.36692548,\n",
       " 0.36714947,\n",
       " 0.37222794,\n",
       " 0.36967167,\n",
       " 0.37276649,\n",
       " 0.36733121,\n",
       " 0.36696759,\n",
       " 0.36808732,\n",
       " 0.367964,\n",
       " 0.3720597,\n",
       " 0.3707681,\n",
       " 0.37176546,\n",
       " 0.37223953,\n",
       " 0.36846694,\n",
       " 0.36900148,\n",
       " 0.36560354,\n",
       " 0.36901215,\n",
       " 0.36683512,\n",
       " 0.3707839,\n",
       " 0.37318113,\n",
       " 0.37498513,\n",
       " 0.37095124,\n",
       " 0.37210292,\n",
       " 0.37378004,\n",
       " 0.36589602,\n",
       " 0.37163067,\n",
       " 0.36798376,\n",
       " 0.37338459,\n",
       " 0.38131407,\n",
       " 0.36630675,\n",
       " 0.36618635,\n",
       " 0.36944836,\n",
       " 0.36863381,\n",
       " 0.36945993,\n",
       " 0.37089714,\n",
       " 0.36902812,\n",
       " 0.36884251,\n",
       " 0.36929286,\n",
       " 0.36850244,\n",
       " 0.36604825,\n",
       " 0.36576456,\n",
       " 0.36841658,\n",
       " 0.36959633,\n",
       " 0.37376618,\n",
       " 0.36794689,\n",
       " 0.37534907,\n",
       " 0.37000507,\n",
       " 0.36972141,\n",
       " 0.37251389,\n",
       " 0.36936039,\n",
       " 0.36592695,\n",
       " 0.36582875,\n",
       " 0.37035659,\n",
       " 0.36613145,\n",
       " 0.3691898,\n",
       " 0.36625984,\n",
       " 0.36589381,\n",
       " 0.37096214,\n",
       " 0.36582479,\n",
       " 0.36927199,\n",
       " 0.36766285,\n",
       " 0.36901879,\n",
       " 0.36893088,\n",
       " 0.3769539,\n",
       " 0.376425,\n",
       " 0.3770858,\n",
       " 0.37792671,\n",
       " 0.37574315,\n",
       " 0.37601584,\n",
       " 0.37334731,\n",
       " 0.3771362,\n",
       " 0.37779641,\n",
       " 0.37638959,\n",
       " 0.3780047,\n",
       " 0.3672047,\n",
       " 0.37333748,\n",
       " 0.37123701,\n",
       " 0.36914805,\n",
       " 0.36706868,\n",
       " 0.36609474,\n",
       " 0.36910498,\n",
       " 0.3681066,\n",
       " 0.36556461,\n",
       " 0.36924976,\n",
       " 0.37026453,\n",
       " 0.37068003,\n",
       " 0.36916605,\n",
       " 0.36929062,\n",
       " 0.36962658,\n",
       " 0.36570174,\n",
       " 0.3711963,\n",
       " 0.37103891,\n",
       " 0.36705625,\n",
       " 0.36845654,\n",
       " 0.36886516,\n",
       " 0.36699444,\n",
       " 0.36637229,\n",
       " 0.36972788,\n",
       " 0.36699152,\n",
       " 0.36932442,\n",
       " 0.37123257,\n",
       " 0.37124568,\n",
       " 0.36686304,\n",
       " 0.37106651,\n",
       " 0.37009889,\n",
       " 0.36861166,\n",
       " 0.36724615,\n",
       " 0.36560041,\n",
       " 0.36911964,\n",
       " 0.36855084,\n",
       " 0.37154028,\n",
       " 0.37515837,\n",
       " 0.37065667,\n",
       " 0.36692709,\n",
       " 0.37157923,\n",
       " 0.37113416,\n",
       " 0.36621827,\n",
       " 0.3745952,\n",
       " 0.37162912,\n",
       " 0.36652064,\n",
       " 0.36951989,\n",
       " 0.3678675,\n",
       " 0.36756906,\n",
       " 0.3674188,\n",
       " 0.36750633,\n",
       " 0.3712568,\n",
       " 0.3751289,\n",
       " 0.36756507,\n",
       " 0.37326255,\n",
       " 0.37814981,\n",
       " 0.36938444,\n",
       " 0.36978167,\n",
       " 0.36855969,\n",
       " 0.37605616,\n",
       " 0.37316149,\n",
       " 0.36975056,\n",
       " 0.37034592,\n",
       " 0.371012,\n",
       " 0.37009686,\n",
       " 0.37663051,\n",
       " 0.3670274,\n",
       " 0.37731501,\n",
       " 0.3703486,\n",
       " 0.37340599,\n",
       " 0.37503052,\n",
       " 0.3754853,\n",
       " 0.37597069,\n",
       " 0.37689564,\n",
       " 0.38071871,\n",
       " 0.37405735,\n",
       " 0.3774181,\n",
       " 0.37331986,\n",
       " 0.37401199,\n",
       " 0.37443411,\n",
       " 0.37479386,\n",
       " 0.37633452,\n",
       " 0.37629336,\n",
       " 0.37570176,\n",
       " 0.37490425,\n",
       " 0.37204123,\n",
       " 0.37474355,\n",
       " 0.37531912,\n",
       " 0.36680102,\n",
       " 0.37026495,\n",
       " 0.36962834,\n",
       " 0.37178123,\n",
       " 0.37388518,\n",
       " 0.37230483,\n",
       " 0.37036392,\n",
       " 0.36725211,\n",
       " 0.37514359,\n",
       " 0.37490088,\n",
       " 0.3723864,\n",
       " 0.37591073,\n",
       " 0.36987346,\n",
       " 0.37185475,\n",
       " 0.37527487,\n",
       " 0.3718532,\n",
       " 0.37324893,\n",
       " 0.36847672,\n",
       " 0.37011823,\n",
       " 0.36762094,\n",
       " 0.37387249,\n",
       " 0.37490982,\n",
       " 0.37443855,\n",
       " 0.37263185,\n",
       " 0.37411854,\n",
       " 0.37418085,\n",
       " 0.3694908,\n",
       " 0.36597168,\n",
       " 0.36589515,\n",
       " 0.36842412,\n",
       " 0.3687515,\n",
       " 0.36913383,\n",
       " 0.37500077,\n",
       " 0.3764711,\n",
       " 0.37029567,\n",
       " 0.36622885,\n",
       " 0.3705866,\n",
       " 0.37007779,\n",
       " 0.37341359,\n",
       " 0.37121943,\n",
       " 0.37084618,\n",
       " 0.36873284,\n",
       " 0.37301114,\n",
       " 0.36943522,\n",
       " 0.37243724,\n",
       " 0.37173712,\n",
       " 0.36741632,\n",
       " 0.36664286,\n",
       " 0.36841238,\n",
       " 0.36630899,\n",
       " 0.3800644,\n",
       " 0.36825573,\n",
       " 0.37870407,\n",
       " 0.3683216,\n",
       " 0.37253913,\n",
       " 0.3754698,\n",
       " 0.36992928,\n",
       " 0.36720181,\n",
       " 0.36847869,\n",
       " 0.36931148,\n",
       " 0.36794558,\n",
       " 0.37104514,\n",
       " 0.36866936,\n",
       " 0.3683973,\n",
       " 0.36816406,\n",
       " 0.36634573,\n",
       " 0.37012953,\n",
       " 0.36601064,\n",
       " 0.366081,\n",
       " 0.36666414,\n",
       " 0.36631426,\n",
       " 0.37054566,\n",
       " 0.37247735,\n",
       " 0.36939996,\n",
       " 0.37054831,\n",
       " 0.36626783,\n",
       " 0.368581,\n",
       " 0.36876103,\n",
       " 0.36968434,\n",
       " 0.3717714,\n",
       " 0.37005153,\n",
       " 0.37414822,\n",
       " 0.37084222,\n",
       " 0.36906275,\n",
       " 0.37154582,\n",
       " 0.37088981,\n",
       " 0.37322399,\n",
       " 0.36650428,\n",
       " 0.37121564,\n",
       " 0.36949325,\n",
       " 0.37597269,\n",
       " 0.36853018,\n",
       " 0.36866847,\n",
       " 0.36889955,\n",
       " 0.3737528,\n",
       " 0.36612353,\n",
       " 0.36956701,\n",
       " 0.36944544,\n",
       " 0.36870795,\n",
       " 0.37207171,\n",
       " 0.3702307,\n",
       " 0.36980566,\n",
       " 0.37218294,\n",
       " 0.37180817,\n",
       " 0.36574024,\n",
       " 0.37194139,\n",
       " 0.36897218,\n",
       " 0.36964056,\n",
       " 0.37312379,\n",
       " 0.36686257,\n",
       " 0.36642236,\n",
       " 0.37187704,\n",
       " 0.36883873,\n",
       " 0.37075877,\n",
       " 0.36969012,\n",
       " 0.36749586,\n",
       " 0.36923733,\n",
       " 0.36924821,\n",
       " 0.37120515,\n",
       " 0.3694157,\n",
       " 0.37083533,\n",
       " 0.36697114,\n",
       " 0.3678602,\n",
       " 0.37121829,\n",
       " 0.36880788,\n",
       " 0.3677828,\n",
       " 0.37887689,\n",
       " 0.36813968,\n",
       " 0.37066421,\n",
       " 0.37238374,\n",
       " 0.37186208,\n",
       " 0.36936393,\n",
       " 0.36956611,\n",
       " 0.36823526,\n",
       " 0.3675755,\n",
       " 0.36874351,\n",
       " 0.37322506,\n",
       " 0.37312177,\n",
       " 0.37771192,\n",
       " 0.3706328,\n",
       " 0.37833303,\n",
       " 0.37785158,\n",
       " 0.37268242,\n",
       " 0.37291613,\n",
       " 0.3791112,\n",
       " 0.37500054,\n",
       " 0.37261224,\n",
       " 0.37643376,\n",
       " 0.37664217,\n",
       " 0.37842315,\n",
       " 0.37562838,\n",
       " 0.37253913,\n",
       " 0.37742659,\n",
       " 0.37768278,\n",
       " 0.37540296,\n",
       " 0.37041998,\n",
       " 0.36903414,\n",
       " 0.36822486,\n",
       " 0.36860189,\n",
       " 0.37230012,\n",
       " 0.36723813,\n",
       " 0.37063688,\n",
       " 0.36626914,\n",
       " 0.37107033,\n",
       " 0.36867979,\n",
       " 0.36877304,\n",
       " 0.3719002,\n",
       " 0.36637363,\n",
       " 0.36959237,\n",
       " 0.36619917,\n",
       " 0.36593187,\n",
       " 0.37078792,\n",
       " 0.37703991,\n",
       " 0.36756507,\n",
       " 0.36577165,\n",
       " 0.36578095,\n",
       " 0.37390685,\n",
       " 0.36657202,\n",
       " 0.37507704,\n",
       " 0.37049586,\n",
       " 0.36577785,\n",
       " 0.37606937,\n",
       " 0.37066045,\n",
       " 0.37378207,\n",
       " 0.36692685,\n",
       " 0.36594602,\n",
       " 0.36630988,\n",
       " 0.3680982,\n",
       " 0.37002018,\n",
       " 0.36821619,\n",
       " 0.36906633,\n",
       " 0.36890849,\n",
       " 0.36598894,\n",
       " 0.36882609,\n",
       " 0.37829646,\n",
       " 0.37201694,\n",
       " 0.3733353,\n",
       " 0.36936438,\n",
       " 0.37761307,\n",
       " 0.36883804,\n",
       " 0.37511769,\n",
       " 0.36943683,\n",
       " 0.36961433,\n",
       " 0.3708818,\n",
       " 0.36871621,\n",
       " 0.36670092,\n",
       " 0.37039664,\n",
       " 0.36832759,\n",
       " 0.36974925,\n",
       " 0.36874264,\n",
       " 0.37148368,\n",
       " 0.36860964,\n",
       " 0.36715174,\n",
       " 0.36785111,\n",
       " 0.36677578,\n",
       " 0.36975569,\n",
       " 0.37170863,\n",
       " 0.36602521,\n",
       " 0.37026429,\n",
       " 0.36932704,\n",
       " 0.3690379,\n",
       " 0.36758813,\n",
       " 0.37089315,\n",
       " 0.3710812,\n",
       " 0.37233624,\n",
       " 0.37170574,\n",
       " 0.37063062,\n",
       " 0.37354529,\n",
       " 0.37032434,\n",
       " 0.36762318,\n",
       " 0.37089807,\n",
       " 0.36718807,\n",
       " 0.36830321,\n",
       " 0.36593363,\n",
       " 0.36813542,\n",
       " 0.3714307,\n",
       " 0.36959076,\n",
       " 0.37056032,\n",
       " 0.36738709,\n",
       " 0.37051204,\n",
       " 0.37043846,\n",
       " 0.3713657,\n",
       " 0.37217915,\n",
       " 0.36604208,\n",
       " 0.3690519,\n",
       " 0.37032747,\n",
       " 0.36895421,\n",
       " 0.38366857,\n",
       " 0.36700746,\n",
       " 0.3676675,\n",
       " 0.36936107,\n",
       " 0.36917713,\n",
       " 0.36955771,\n",
       " 0.37793452,\n",
       " 0.36675894,\n",
       " 0.36937591,\n",
       " 0.36695719,\n",
       " 0.37003332,\n",
       " 0.37170458,\n",
       " 0.37697047,\n",
       " 0.37224931,\n",
       " 0.36747462,\n",
       " 0.36996129,\n",
       " 0.37566376,\n",
       " 0.37750387,\n",
       " 0.37267753,\n",
       " 0.37210068,\n",
       " 0.37472031,\n",
       " 0.3773661,\n",
       " 0.37366733,\n",
       " 0.37667376,\n",
       " 0.37890697,\n",
       " 0.37283874,\n",
       " 0.36999151,\n",
       " 0.36723968,\n",
       " 0.36698756,\n",
       " 0.36722818,\n",
       " 0.37159348,\n",
       " 0.36689228,\n",
       " 0.3692911,\n",
       " 0.36923268,\n",
       " 0.3693608,\n",
       " 0.36813301,\n",
       " 0.36674476,\n",
       " 0.37484899,\n",
       " 0.37433383,\n",
       " 0.36931685,\n",
       " 0.37385932,\n",
       " 0.39485043,\n",
       " 0.37345243,\n",
       " 0.37204969,\n",
       " 0.38497284,\n",
       " 0.37709165,\n",
       " 0.37178859,\n",
       " 0.36759526,\n",
       " 0.37006176,\n",
       " 0.36825058,\n",
       " 0.3747234,\n",
       " 0.37078503,\n",
       " 0.37205815,\n",
       " 0.37391686,\n",
       " 0.37340623,\n",
       " 0.37476745,\n",
       " 0.37210029,\n",
       " 0.37623405,\n",
       " 0.37141156,\n",
       " 0.37460503,\n",
       " 0.37554252,\n",
       " 0.37930992,\n",
       " 0.37211719,\n",
       " 0.37317419,\n",
       " 0.3761,\n",
       " 0.37772715,\n",
       " 0.37706634,\n",
       " 0.36988747,\n",
       " 0.37111148,\n",
       " 0.36647815,\n",
       " 0.36680016,\n",
       " 0.37153023,\n",
       " 0.36740771,\n",
       " 0.36802143,\n",
       " 0.36941969,\n",
       " 0.3672049,\n",
       " 0.3728452,\n",
       " 0.37679419,\n",
       " 0.36737376,\n",
       " 0.37151399,\n",
       " 0.36690357,\n",
       " 0.37047666,\n",
       " 0.36817184,\n",
       " 0.37559241,\n",
       " 0.36982214,\n",
       " 0.37073073,\n",
       " 0.37248895,\n",
       " 0.3696219,\n",
       " 0.36809775,\n",
       " 0.36664066,\n",
       " 0.37033677,\n",
       " 0.36971545,\n",
       " 0.36985725,\n",
       " 0.37024984,\n",
       " 0.37077188,\n",
       " 0.36556062,\n",
       " 0.37334129,\n",
       " 0.36816251,\n",
       " 0.36775512,\n",
       " 0.36906302,\n",
       " 0.37336475,\n",
       " 0.37132093,\n",
       " 0.36942971,\n",
       " 0.37317106,\n",
       " 0.36889356,\n",
       " 0.36736336,\n",
       " 0.36977014,\n",
       " 0.36800063,\n",
       " 0.37027299,\n",
       " 0.37250674,\n",
       " 0.37101221,\n",
       " 0.3693966,\n",
       " 0.3661828,\n",
       " 0.37221947,\n",
       " 0.37264612,\n",
       " 0.36891222,\n",
       " 0.3670921,\n",
       " 0.36853659,\n",
       " 0.36696827,\n",
       " 0.36578095,\n",
       " 0.36994123,\n",
       " 0.37237415,\n",
       " 0.37070492,\n",
       " 0.37335044,\n",
       " 0.36828634,\n",
       " 0.37486735,\n",
       " 0.37186518,\n",
       " 0.37934783,\n",
       " 0.37433496,\n",
       " 0.37386107,\n",
       " 0.37535223,\n",
       " 0.37445086,\n",
       " 0.3686969,\n",
       " 0.38031292,\n",
       " 0.37421834,\n",
       " 0.37611479,\n",
       " 0.37645274,\n",
       " 0.37391984,\n",
       " 0.37600783,\n",
       " 0.37300622,\n",
       " 0.37520936,\n",
       " 0.37313113,\n",
       " 0.37164494,\n",
       " 0.37023851,\n",
       " 0.37104091,\n",
       " 0.37266508,\n",
       " 0.3700324,\n",
       " 0.37059751,\n",
       " 0.36925244,\n",
       " 0.36858809,\n",
       " 0.3859542,\n",
       " 0.36713642,\n",
       " 0.37713933,\n",
       " 0.37366912,\n",
       " 0.37343547,\n",
       " 0.37349394,\n",
       " 0.37417054,\n",
       " 0.36762807,\n",
       " 0.37810943,\n",
       " 0.37423351,\n",
       " 0.37406647,\n",
       " 0.37384972,\n",
       " 0.37375417,\n",
       " 0.36698222,\n",
       " 0.36826259,\n",
       " 0.36723328,\n",
       " 0.3688021,\n",
       " 0.36937636,\n",
       " 0.37138796,\n",
       " 0.36711517,\n",
       " 0.36747819,\n",
       " 0.36814168,\n",
       " 0.3719472,\n",
       " 0.36938038,\n",
       " 0.37039617,\n",
       " 0.373739,\n",
       " 0.37215036,\n",
       " 0.36782497,\n",
       " 0.37352806,\n",
       " 0.36935598,\n",
       " 0.37451592,\n",
       " 0.3845616,\n",
       " 0.37446001,\n",
       " 0.37603712,\n",
       " 0.38224998,\n",
       " 0.373041,\n",
       " 0.3741757,\n",
       " 0.37216195,\n",
       " 0.37737706,\n",
       " 0.37455297,\n",
       " 0.37997091,\n",
       " 0.37439147,\n",
       " 0.37607718,\n",
       " 0.37274396,\n",
       " 0.37325361,\n",
       " 0.37057453,\n",
       " 0.37571877,\n",
       " 0.37350619,\n",
       " 0.38417408,\n",
       " 0.3736968,\n",
       " 0.37009707,\n",
       " 0.37021786,\n",
       " 0.36804405,\n",
       " 0.37423304,\n",
       " 0.36921269,\n",
       " 0.36611819,\n",
       " 0.37085712,\n",
       " 0.36739084,\n",
       " 0.36825594,\n",
       " 0.36607435,\n",
       " 0.37169525,\n",
       " 0.37230012,\n",
       " 0.36647058,\n",
       " 0.37302789,\n",
       " 0.3784028,\n",
       " 0.36581945,\n",
       " 0.37113842,\n",
       " 0.37232691,\n",
       " 0.36913007,\n",
       " 0.36661762,\n",
       " 0.36883229,\n",
       " 0.36582652,\n",
       " 0.37139753,\n",
       " 0.3691192,\n",
       " 0.37612039,\n",
       " 0.3708522,\n",
       " 0.37161863,\n",
       " 0.37103069,\n",
       " 0.36873683,\n",
       " 0.36860055,\n",
       " 0.37258145,\n",
       " 0.36629394,\n",
       " 0.36838308,\n",
       " 0.36876103,\n",
       " 0.36671376,\n",
       " 0.36984345,\n",
       " 0.36789966,\n",
       " 0.36853793,\n",
       " 0.37345442,\n",
       " 0.36962476,\n",
       " 0.37169084,\n",
       " 0.3782348,\n",
       " 0.37364992,\n",
       " 0.36610445,\n",
       " 0.36638248,\n",
       " 0.36951235,\n",
       " 0.37073117,\n",
       " 0.37809843,\n",
       " 0.38751084,\n",
       " 0.38050312,\n",
       " 0.38008618,\n",
       " 0.38792083,\n",
       " 0.38657531,\n",
       " 0.37756309,\n",
       " 0.37579572,\n",
       " 0.38274756,\n",
       " 0.37722471,\n",
       " 0.3749105,\n",
       " 0.3939698,\n",
       " 0.37737644,\n",
       " 0.38600209,\n",
       " 0.37252569,\n",
       " 0.37980601,\n",
       " 0.38734445,\n",
       " 0.3703686,\n",
       " 0.37625936,\n",
       " 0.37780336,\n",
       " 0.39708099,\n",
       " 0.37812421,\n",
       " 0.38092014,\n",
       " 0.37077701,\n",
       " 0.37966806,\n",
       " 0.37770852,\n",
       " 0.36924088,\n",
       " 0.37376311,\n",
       " 0.37797061,\n",
       " 0.36990169,\n",
       " 0.36960834,\n",
       " 0.38462475,\n",
       " 0.37013775,\n",
       " 0.37508059,\n",
       " 0.36798948,\n",
       " 0.36950102,\n",
       " 0.36868024,\n",
       " 0.36926731,\n",
       " 0.38058019,\n",
       " 0.37653896,\n",
       " 0.38338742,\n",
       " 0.37964115,\n",
       " 0.37655443,\n",
       " 0.39312264,\n",
       " 0.3792356,\n",
       " 0.37745932,\n",
       " 0.37513492,\n",
       " 0.37923941,\n",
       " 0.37336004,\n",
       " 0.36960742,\n",
       " 0.38031742,\n",
       " 0.3679336,\n",
       " 0.37125859,\n",
       " 0.37076476,\n",
       " 0.394896,\n",
       " 0.37021896,\n",
       " 0.3721925,\n",
       " 0.37076101,\n",
       " 0.37828952,\n",
       " 0.37288469,\n",
       " 0.37091562,\n",
       " 0.37583244,\n",
       " 0.36916405,\n",
       " 0.37594023,\n",
       " 0.36785421,\n",
       " 0.36582875,\n",
       " 0.36911786,\n",
       " 0.37771392,\n",
       " 0.37658057,\n",
       " 0.37095234,\n",
       " 0.37449196,\n",
       " 0.36901614,\n",
       " 0.36749771,\n",
       " 0.36729023,\n",
       " 0.37635493,\n",
       " 0.37369946,\n",
       " 0.36769101,\n",
       " 0.37193069,\n",
       " 0.37424424,\n",
       " 0.36758906,\n",
       " 0.37118182,\n",
       " 0.37631196,\n",
       " 0.37047467,\n",
       " 0.3668431,\n",
       " 0.36696184,\n",
       " 0.36868754,\n",
       " 0.37122947,\n",
       " 0.37066087,\n",
       " 0.36804363,\n",
       " 0.36835778,\n",
       " 0.36755821,\n",
       " 0.3783108,\n",
       " 0.3702763,\n",
       " 0.36882168,\n",
       " 0.39372805,\n",
       " 0.37083909,\n",
       " 0.37137347,\n",
       " 0.37325719,\n",
       " 0.37381285,\n",
       " 0.3669377,\n",
       " 0.39373419,\n",
       " 0.38037428,\n",
       " 0.36965811,\n",
       " 0.37388363,\n",
       " 0.36772558,\n",
       " 0.36677444,\n",
       " 0.36929595,\n",
       " 0.36694279,\n",
       " 0.37402585,\n",
       " 0.36653796,\n",
       " 0.36605003,\n",
       " 0.36623994,\n",
       " 0.3657093,\n",
       " 0.36812726,\n",
       " 0.36841238,\n",
       " 0.37696865,\n",
       " 0.37044954,\n",
       " 0.36880568,\n",
       " 0.36999908,\n",
       " 0.37434611,\n",
       " 0.36957014,\n",
       " 0.36919379,\n",
       " 0.37121877,\n",
       " 0.37270096,\n",
       " 0.36565837,\n",
       " 0.36826414,\n",
       " 0.36690646,\n",
       " 0.37157676,\n",
       " 0.36833113,\n",
       " 0.36864361,\n",
       " 0.3681725,\n",
       " 0.36676648,\n",
       " 0.37020183,\n",
       " 0.36696386,\n",
       " 0.39373374,\n",
       " 0.36879364,\n",
       " 0.3713136,\n",
       " 0.37681279,\n",
       " 0.36563984,\n",
       " 0.37286931,\n",
       " 0.39373419,\n",
       " 0.36797443,\n",
       " 0.36645645,\n",
       " 0.37013444,\n",
       " 0.37335938,\n",
       " 0.37336633,\n",
       " 0.36711958,\n",
       " 0.36580664,\n",
       " 0.37359682,\n",
       " 0.36682716,\n",
       " 0.36892799,\n",
       " 0.37006599,\n",
       " 0.37093744,\n",
       " 0.36681342,\n",
       " 0.36843854,\n",
       " 0.37123302,\n",
       " 0.37322214,\n",
       " 0.37219962,\n",
       " 0.37792265,\n",
       " 0.36659905,\n",
       " 0.36558008,\n",
       " 0.36825344,\n",
       " 0.3687768,\n",
       " 0.37713826,\n",
       " 0.38035116,\n",
       " 0.36910051,\n",
       " 0.36913052,\n",
       " 0.36937419,\n",
       " 0.3716113,\n",
       " 0.3689442,\n",
       " 0.36947456,\n",
       " 0.36823753,\n",
       " 0.37061372,\n",
       " 0.3681927,\n",
       " 0.36999685,\n",
       " 0.36729422,\n",
       " 0.37573487,\n",
       " 0.36675054,\n",
       " 0.37610698,\n",
       " 0.37631842,\n",
       " 0.38244885,\n",
       " 0.37576416,\n",
       " 0.37984869,\n",
       " 0.390174,\n",
       " 0.37589371,\n",
       " 0.38125378,\n",
       " 0.40244281,\n",
       " 0.37812868,\n",
       " 0.37103334,\n",
       " 0.37329289,\n",
       " 0.37017643,\n",
       " 0.37384972,\n",
       " 0.37190533,\n",
       " 0.36861873,\n",
       " 0.3747339,\n",
       " 0.37464145,\n",
       " 0.37482357,\n",
       " 0.3773585,\n",
       " 0.37430102,\n",
       " 0.37233201,\n",
       " 0.3710672,\n",
       " 0.37379569,\n",
       " 0.37075034,\n",
       " 0.37020093,\n",
       " 0.37120473,\n",
       " 0.37392092,\n",
       " 0.37230125,\n",
       " 0.38179269,\n",
       " 0.38336983,\n",
       " 0.37556869,\n",
       " 0.37379211,\n",
       " 0.36805826]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[preds[i][1] for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(index 0 : -100 (NaN), index 1 : 0 (not valid), index 2 : 1 (valid))  \n",
    "index 0 : 0 (not valid), index 1 : 1 (valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [np.argmax(preds[i]) for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student = 142954.0\n",
    "X_k_ = X[student]\n",
    "X_k, ex_k = X_k_[X_k_!=-100], X_k_[X_k_!=-100].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 15\n",
    "full = 20\n",
    "\n",
    "X_train = X_k.values.reshape(full,1)\n",
    "y_train = X_k[:full].values.reshape(full,1)\n",
    "\n",
    "A_train = A[A.index.isin(idx_full)][sorted(idx_full)]\n",
    "A_train = scipy.sparse.csr_matrix(A_train.values)\n",
    "\n",
    "train_mask = sample_mask(range(15), A_train.shape[0]).reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_train.shape, X_train.shape, y_train.shape, train_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize X_train\n",
    "X_train = X_train/X_train.sum()\n",
    "X_train = X_train.reshape((len(X_train), 1))\n",
    "\n",
    "A_ = preprocess_adj(A_train, True)\n",
    "support = 1\n",
    "graph = [X_train, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "X_in = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y_train.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = model.fit(graph, y_train, #sample_weight=train_mask, \n",
    "          batch_size=A_train.shape[0], epochs=1, shuffle=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:49:30.604314Z",
     "start_time": "2017-12-28T18:49:30.597102Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student = 142954.0 # np.random.choice(X.columns)\n",
    "X_k_ = X[student]\n",
    "X_k, ex_k = X_k_[X_k_!=-100], X_k_[X_k_!=-100].index\n",
    "c = 15\n",
    "assert c + 1< len(X_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    if x == 0:\n",
    "        return [1,0]\n",
    "    elif x == 1:\n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:00:07.339350Z",
     "start_time": "2017-12-28T19:00:07.326173Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 15\n",
    "idx_train = random.sample(list(ex_k), n_samples)\n",
    "idx_test = random.sample([k for k in ex_k if k not in idx_train], 1)\n",
    "idx_full = np.hstack((idx_train,idx_test))\n",
    "X_train = X_k.loc[idx_full].values\n",
    "#X_train = to_categorical(X_train)\n",
    "#y_train = X_k.loc[idx_test]\n",
    "#y_train = np.array(y_train).reshape((len(y_train), 1))\n",
    "y_train = np.zeros((X_train.shape[0],2))\n",
    "y_train[n_samples] = one_hot(X_train[n_samples])\n",
    "A_train = A[A.index.isin(idx_full)][sorted(idx_full)]\n",
    "A_train = scipy.sparse.csr_matrix(A_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:58:46.305179Z",
     "start_time": "2017-12-28T18:58:46.301657Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_train.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:00:21.748166Z",
     "start_time": "2017-12-28T19:00:21.745513Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mask = np.array(list(map(bool, np.hstack((np.ones(n_samples), [0])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:00:42.405322Z",
     "start_time": "2017-12-28T19:00:42.402219Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize X_train\n",
    "X_train = X_train/X_train.sum()\n",
    "X_train = X_train.reshape((len(X_train), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:00:44.698860Z",
     "start_time": "2017-12-28T19:00:44.689536Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A_ = preprocess_adj(A_train, True)\n",
    "support = 1\n",
    "graph = [X_train, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:01:03.439748Z",
     "start_time": "2017-12-28T19:01:03.330056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y_train.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:01:04.379941Z",
     "start_time": "2017-12-28T19:01:03.642338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.fit(graph, y_train, sample_weight=train_mask,\n",
    "          batch_size=A_train.shape[0], epochs=1, shuffle=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:01:05.569414Z",
     "start_time": "2017-12-28T19:01:05.521980Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(graph, batch_size=A_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T19:01:06.526325Z",
     "start_time": "2017-12-28T19:01:06.521601Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
