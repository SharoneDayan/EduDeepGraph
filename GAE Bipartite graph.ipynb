{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from optimizer import OptimizerAE, OptimizerVAE\n",
    "from input_data import load_data, parse_index_file\n",
    "from model import GCNModelAE, GCNModelVAE\n",
    "from preprocessing import preprocess_graph, construct_feed_dict, sparse_to_tuple, mask_test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 16, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0., 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Model string.')\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')\n",
    "flags.DEFINE_integer('features', 1, 'Whether to use features (1) or not (0).')\n",
    "\n",
    "model_str = FLAGS.model\n",
    "dataset_str = FLAGS.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_hdf('../../history_small.hdf', key='hist') \n",
    "A = pd.read_pickle('../../adjacency_small.pkl') \n",
    "A = A[sorted(A.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X==1] = 2 #10 #valid\n",
    "X[X==0] = 1 #not valid\n",
    "X[X==-100] = 0 #missing\n",
    "X = X.astype(float)\n",
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A[A<=0.75] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj = scipy.sparse.csr_matrix(A.values)\n",
    "features = scipy.sparse.csr_matrix(features)#np.asmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 290) (878, 878)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(features.shape, adj.shape)\n",
    "print(type(features), type(adj)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<878x878 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 393718 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<878x290 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8821 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj.eliminate_zeros()\n",
    "# Check that diag is zero:\n",
    "assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "adj_triu = sp.triu(adj)\n",
    "adj_tuple = sparse_to_tuple(adj_triu)\n",
    "edges = adj_tuple[0]\n",
    "edges_all = sparse_to_tuple(adj)[0]\n",
    "num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "\n",
    "all_edge_idx = list(range(edges.shape[0]))\n",
    "np.random.shuffle(all_edge_idx)\n",
    "val_edge_idx = all_edge_idx[:num_val]\n",
    "test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "test_edges = edges[test_edge_idx]\n",
    "val_edges = edges[val_edge_idx]\n",
    "train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ismember(a, b, tol=5):\n",
    "    rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "    return (np.all(np.any(rows_close, axis=-1), axis=-1) and\n",
    "            np.all(np.any(rows_close, axis=0), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928 / 3928"
     ]
    }
   ],
   "source": [
    "test_edges_false = []\n",
    "while len(test_edges_false) < len(test_edges):\n",
    "    idx_i = np.random.randint(0, adj.shape[0])\n",
    "    idx_j = np.random.randint(0, adj.shape[0])\n",
    "    if idx_i == idx_j:\n",
    "        continue\n",
    "    if ismember([idx_i, idx_j], edges_all):\n",
    "        continue\n",
    "    if test_edges_false:\n",
    "        if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "            continue\n",
    "    test_edges_false.append([idx_i, idx_j])\n",
    "    print('\\r%d / %d' % (len(test_edges_false), len(test_edges)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928 / 3928"
     ]
    }
   ],
   "source": [
    "val_edges_false = []\n",
    "while len(val_edges_false) < len(val_edges):\n",
    "    idx_i = np.random.randint(0, adj.shape[0])\n",
    "    idx_j = np.random.randint(0, adj.shape[0])\n",
    "    if idx_i == idx_j:\n",
    "        continue\n",
    "    if ismember([idx_i, idx_j], train_edges):\n",
    "        continue\n",
    "    if ismember([idx_j, idx_i], train_edges):\n",
    "        continue\n",
    "    if ismember([idx_i, idx_j], val_edges):\n",
    "        continue\n",
    "    if ismember([idx_j, idx_i], val_edges):\n",
    "        continue\n",
    "    if val_edges_false:\n",
    "        if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "            continue\n",
    "    val_edges_false.append([idx_i, idx_j])\n",
    "    print('\\r%d / %d' % (len(val_edges_false), len(val_edges)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert ~ismember(test_edges_false, edges_all)\n",
    "#assert ~ismember(val_edges_false, edges_all)\n",
    "#assert ~ismember(val_edges, train_edges)\n",
    "#assert ~ismember(test_edges, train_edges)\n",
    "#assert ~ismember(val_edges, test_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.ones(train_edges.shape[0])\n",
    "# Re-build adj matrix\n",
    "adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "adj_train = adj_train + adj_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "#adj = adj_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if FLAGS.features == 0:\n",
    "    features = sp.identity(features.shape[0])  # featureless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = adj.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = None\n",
    "if model_str == 'gcn_ae':\n",
    "    model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "elif model_str == 'gcn_vae':\n",
    "    model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
    "\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    if model_str == 'gcn_ae':\n",
    "        opt = OptimizerAE(preds=model.reconstructions,\n",
    "                          labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                      validate_indices=False), [-1]),\n",
    "                          pos_weight=pos_weight,\n",
    "                          norm=norm)\n",
    "    elif model_str == 'gcn_vae':\n",
    "        opt = OptimizerVAE(preds=model.reconstructions,\n",
    "                           labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                       validate_indices=False), [-1]),\n",
    "                           model=model, num_nodes=num_nodes,\n",
    "                           pos_weight=pos_weight,\n",
    "                           norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "acc_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_roc_score(edges_pos, edges_neg, emb=None):\n",
    "    if emb is None:\n",
    "        feed_dict.update({placeholders['dropout']: 0})\n",
    "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.66534 train_acc= 0.49035 val_roc= 0.65255 val_ap= 0.61621 time= 0.69629\n",
      "Epoch: 0002 train_loss= 0.84849 train_acc= 0.49035 val_roc= 0.65392 val_ap= 0.61654 time= 0.68130\n",
      "Epoch: 0003 train_loss= 0.65636 train_acc= 0.49035 val_roc= 0.65771 val_ap= 0.61716 time= 0.74666\n",
      "Epoch: 0004 train_loss= 0.68005 train_acc= 0.49035 val_roc= 0.65709 val_ap= 0.61649 time= 0.58731\n",
      "Epoch: 0005 train_loss= 0.67681 train_acc= 0.49035 val_roc= 0.65459 val_ap= 0.61586 time= 0.57502\n",
      "Epoch: 0006 train_loss= 0.65980 train_acc= 0.49035 val_roc= 0.65399 val_ap= 0.61583 time= 0.56953\n",
      "Epoch: 0007 train_loss= 0.66946 train_acc= 0.49035 val_roc= 0.65410 val_ap= 0.61573 time= 0.57637\n",
      "Epoch: 0008 train_loss= 0.67308 train_acc= 0.49035 val_roc= 0.65435 val_ap= 0.61569 time= 0.56157\n",
      "Epoch: 0009 train_loss= 0.66303 train_acc= 0.49035 val_roc= 0.65470 val_ap= 0.61570 time= 0.56931\n",
      "Epoch: 0010 train_loss= 0.65818 train_acc= 0.49035 val_roc= 0.65497 val_ap= 0.61565 time= 0.68519\n",
      "Epoch: 0011 train_loss= 0.65987 train_acc= 0.49035 val_roc= 0.65504 val_ap= 0.61572 time= 0.59905\n",
      "Epoch: 0012 train_loss= 0.66256 train_acc= 0.49035 val_roc= 0.65481 val_ap= 0.61576 time= 0.59411\n",
      "Epoch: 0013 train_loss= 0.66334 train_acc= 0.49035 val_roc= 0.65437 val_ap= 0.61584 time= 0.58067\n",
      "Epoch: 0014 train_loss= 0.66194 train_acc= 0.49035 val_roc= 0.65383 val_ap= 0.61592 time= 0.57430\n",
      "Epoch: 0015 train_loss= 0.65932 train_acc= 0.49035 val_roc= 0.65326 val_ap= 0.61613 time= 0.57219\n",
      "Epoch: 0016 train_loss= 0.65715 train_acc= 0.49035 val_roc= 0.65279 val_ap= 0.61614 time= 0.57044\n",
      "Epoch: 0017 train_loss= 0.65694 train_acc= 0.49035 val_roc= 0.65244 val_ap= 0.61610 time= 0.61969\n",
      "Epoch: 0018 train_loss= 0.65842 train_acc= 0.49035 val_roc= 0.65216 val_ap= 0.61614 time= 0.63003\n",
      "Epoch: 0019 train_loss= 0.65940 train_acc= 0.49035 val_roc= 0.65194 val_ap= 0.61619 time= 0.59526\n",
      "Epoch: 0020 train_loss= 0.65845 train_acc= 0.49035 val_roc= 0.65171 val_ap= 0.61622 time= 0.56742\n",
      "Epoch: 0021 train_loss= 0.65670 train_acc= 0.49035 val_roc= 0.65149 val_ap= 0.61629 time= 0.57635\n",
      "Epoch: 0022 train_loss= 0.65576 train_acc= 0.49035 val_roc= 0.65124 val_ap= 0.61643 time= 0.66916\n",
      "Epoch: 0023 train_loss= 0.65587 train_acc= 0.49035 val_roc= 0.65099 val_ap= 0.61654 time= 0.59172\n",
      "Epoch: 0024 train_loss= 0.65636 train_acc= 0.49035 val_roc= 0.65070 val_ap= 0.61663 time= 0.58432\n",
      "Epoch: 0025 train_loss= 0.65655 train_acc= 0.49035 val_roc= 0.65039 val_ap= 0.61668 time= 0.57223\n",
      "Epoch: 0026 train_loss= 0.65617 train_acc= 0.49035 val_roc= 0.65009 val_ap= 0.61668 time= 0.57383\n",
      "Epoch: 0027 train_loss= 0.65537 train_acc= 0.49035 val_roc= 0.64981 val_ap= 0.61651 time= 0.62188\n",
      "Epoch: 0028 train_loss= 0.65460 train_acc= 0.49035 val_roc= 0.64954 val_ap= 0.61643 time= 0.64068\n",
      "Epoch: 0029 train_loss= 0.65426 train_acc= 0.49035 val_roc= 0.64929 val_ap= 0.61650 time= 0.59673\n",
      "Epoch: 0030 train_loss= 0.65440 train_acc= 0.49035 val_roc= 0.64903 val_ap= 0.61643 time= 0.56714\n",
      "Epoch: 0031 train_loss= 0.65457 train_acc= 0.49035 val_roc= 0.64878 val_ap= 0.61647 time= 0.58144\n",
      "Epoch: 0032 train_loss= 0.65429 train_acc= 0.49035 val_roc= 0.64845 val_ap= 0.61626 time= 0.58016\n",
      "Epoch: 0033 train_loss= 0.65365 train_acc= 0.49035 val_roc= 0.64813 val_ap= 0.61609 time= 0.56622\n",
      "Epoch: 0034 train_loss= 0.65310 train_acc= 0.49035 val_roc= 0.64779 val_ap= 0.61603 time= 0.57671\n",
      "Epoch: 0035 train_loss= 0.65287 train_acc= 0.49035 val_roc= 0.64745 val_ap= 0.61581 time= 0.63560\n",
      "Epoch: 0036 train_loss= 0.65282 train_acc= 0.49035 val_roc= 0.64708 val_ap= 0.61550 time= 0.64879\n",
      "Epoch: 0037 train_loss= 0.65271 train_acc= 0.49035 val_roc= 0.64676 val_ap= 0.61536 time= 0.75477\n",
      "Epoch: 0038 train_loss= 0.65237 train_acc= 0.49035 val_roc= 0.64643 val_ap= 0.61506 time= 0.63049\n",
      "Epoch: 0039 train_loss= 0.65186 train_acc= 0.49035 val_roc= 0.64613 val_ap= 0.61475 time= 0.73833\n",
      "Epoch: 0040 train_loss= 0.65141 train_acc= 0.49035 val_roc= 0.64583 val_ap= 0.61443 time= 0.68550\n",
      "Epoch: 0041 train_loss= 0.65112 train_acc= 0.49035 val_roc= 0.64550 val_ap= 0.61399 time= 0.66440\n",
      "Epoch: 0042 train_loss= 0.65096 train_acc= 0.49035 val_roc= 0.64513 val_ap= 0.61354 time= 0.57451\n",
      "Epoch: 0043 train_loss= 0.65069 train_acc= 0.49035 val_roc= 0.64472 val_ap= 0.61306 time= 0.59190\n",
      "Epoch: 0044 train_loss= 0.65022 train_acc= 0.49035 val_roc= 0.64422 val_ap= 0.61239 time= 0.56695\n",
      "Epoch: 0045 train_loss= 0.64972 train_acc= 0.49035 val_roc= 0.64369 val_ap= 0.61180 time= 0.58171\n",
      "Epoch: 0046 train_loss= 0.64932 train_acc= 0.49035 val_roc= 0.64314 val_ap= 0.61106 time= 0.58166\n",
      "Epoch: 0047 train_loss= 0.64900 train_acc= 0.49035 val_roc= 0.64259 val_ap= 0.61050 time= 0.57608\n",
      "Epoch: 0048 train_loss= 0.64864 train_acc= 0.49035 val_roc= 0.64208 val_ap= 0.61006 time= 0.71607\n",
      "Epoch: 0049 train_loss= 0.64814 train_acc= 0.49035 val_roc= 0.64159 val_ap= 0.60959 time= 0.64090\n",
      "Epoch: 0050 train_loss= 0.64759 train_acc= 0.49035 val_roc= 0.64107 val_ap= 0.60922 time= 0.60620\n",
      "Epoch: 0051 train_loss= 0.64710 train_acc= 0.49035 val_roc= 0.64052 val_ap= 0.60878 time= 0.63479\n",
      "Epoch: 0052 train_loss= 0.64669 train_acc= 0.49035 val_roc= 0.63993 val_ap= 0.60833 time= 0.82780\n",
      "Epoch: 0053 train_loss= 0.64628 train_acc= 0.49035 val_roc= 0.63928 val_ap= 0.60777 time= 0.74525\n",
      "Epoch: 0054 train_loss= 0.64577 train_acc= 0.49035 val_roc= 0.63859 val_ap= 0.60734 time= 0.72042\n",
      "Epoch: 0055 train_loss= 0.64518 train_acc= 0.49035 val_roc= 0.63789 val_ap= 0.60696 time= 0.61069\n",
      "Epoch: 0056 train_loss= 0.64466 train_acc= 0.49035 val_roc= 0.63720 val_ap= 0.60657 time= 0.61061\n",
      "Epoch: 0057 train_loss= 0.64419 train_acc= 0.49933 val_roc= 0.63659 val_ap= 0.60612 time= 0.57236\n",
      "Epoch: 0058 train_loss= 0.64363 train_acc= 0.51201 val_roc= 0.63608 val_ap= 0.60587 time= 0.57398\n",
      "Epoch: 0059 train_loss= 0.64299 train_acc= 0.51229 val_roc= 0.63563 val_ap= 0.60560 time= 0.57779\n",
      "Epoch: 0060 train_loss= 0.64239 train_acc= 0.51234 val_roc= 0.63517 val_ap= 0.60535 time= 0.57687\n",
      "Epoch: 0061 train_loss= 0.64181 train_acc= 0.51418 val_roc= 0.63462 val_ap= 0.60493 time= 0.57165\n",
      "Epoch: 0062 train_loss= 0.64118 train_acc= 0.51618 val_roc= 0.63400 val_ap= 0.60449 time= 0.58019\n",
      "Epoch: 0063 train_loss= 0.64054 train_acc= 0.51847 val_roc= 0.63332 val_ap= 0.60404 time= 0.57744\n",
      "Epoch: 0064 train_loss= 0.63986 train_acc= 0.52374 val_roc= 0.63267 val_ap= 0.60341 time= 0.57521\n",
      "Epoch: 0065 train_loss= 0.63924 train_acc= 0.52689 val_roc= 0.63206 val_ap= 0.60296 time= 0.57998\n",
      "Epoch: 0066 train_loss= 0.63858 train_acc= 0.52770 val_roc= 0.63158 val_ap= 0.60253 time= 0.78492\n",
      "Epoch: 0067 train_loss= 0.63792 train_acc= 0.52842 val_roc= 0.63116 val_ap= 0.60218 time= 0.75636\n",
      "Epoch: 0068 train_loss= 0.63727 train_acc= 0.52931 val_roc= 0.63071 val_ap= 0.60172 time= 0.61784\n",
      "Epoch: 0069 train_loss= 0.63668 train_acc= 0.53164 val_roc= 0.63024 val_ap= 0.60131 time= 0.64129\n",
      "Epoch: 0070 train_loss= 0.63606 train_acc= 0.53645 val_roc= 0.62978 val_ap= 0.60089 time= 0.60941\n",
      "Epoch: 0071 train_loss= 0.63545 train_acc= 0.54076 val_roc= 0.62934 val_ap= 0.60053 time= 0.60078\n",
      "Epoch: 0072 train_loss= 0.63484 train_acc= 0.54462 val_roc= 0.62895 val_ap= 0.60020 time= 0.60522\n",
      "Epoch: 0073 train_loss= 0.63422 train_acc= 0.54714 val_roc= 0.62866 val_ap= 0.59993 time= 0.59889\n",
      "Epoch: 0074 train_loss= 0.63355 train_acc= 0.54878 val_roc= 0.62842 val_ap= 0.59963 time= 0.60599\n",
      "Epoch: 0075 train_loss= 0.63287 train_acc= 0.55071 val_roc= 0.62822 val_ap= 0.59934 time= 0.60113\n",
      "Epoch: 0076 train_loss= 0.63217 train_acc= 0.55515 val_roc= 0.62802 val_ap= 0.59902 time= 0.59847\n",
      "Epoch: 0077 train_loss= 0.63144 train_acc= 0.56265 val_roc= 0.62786 val_ap= 0.59873 time= 0.59752\n",
      "Epoch: 0078 train_loss= 0.63067 train_acc= 0.56898 val_roc= 0.62780 val_ap= 0.59853 time= 0.60202\n",
      "Epoch: 0079 train_loss= 0.62988 train_acc= 0.57479 val_roc= 0.62779 val_ap= 0.59844 time= 0.81120\n",
      "Epoch: 0080 train_loss= 0.62904 train_acc= 0.57993 val_roc= 0.62783 val_ap= 0.59826 time= 0.75263\n",
      "Epoch: 0081 train_loss= 0.62816 train_acc= 0.58659 val_roc= 0.62792 val_ap= 0.59817 time= 0.80547\n",
      "Epoch: 0082 train_loss= 0.62723 train_acc= 0.59326 val_roc= 0.62812 val_ap= 0.59812 time= 0.75646\n",
      "Epoch: 0083 train_loss= 0.62624 train_acc= 0.59964 val_roc= 0.62842 val_ap= 0.59803 time= 0.68054\n",
      "Epoch: 0084 train_loss= 0.62518 train_acc= 0.60813 val_roc= 0.62864 val_ap= 0.59784 time= 0.66375\n",
      "Epoch: 0085 train_loss= 0.62407 train_acc= 0.61918 val_roc= 0.62885 val_ap= 0.59769 time= 0.59630\n",
      "Epoch: 0086 train_loss= 0.62287 train_acc= 0.62596 val_roc= 0.62905 val_ap= 0.59760 time= 0.62719\n",
      "Epoch: 0087 train_loss= 0.62158 train_acc= 0.62936 val_roc= 0.62923 val_ap= 0.59745 time= 0.78671\n",
      "Epoch: 0088 train_loss= 0.62019 train_acc= 0.63482 val_roc= 0.62944 val_ap= 0.59736 time= 0.79441\n",
      "Epoch: 0089 train_loss= 0.61871 train_acc= 0.63783 val_roc= 0.62968 val_ap= 0.59733 time= 0.81902\n",
      "Epoch: 0090 train_loss= 0.61711 train_acc= 0.64197 val_roc= 0.62998 val_ap= 0.59737 time= 0.77258\n",
      "Epoch: 0091 train_loss= 0.61537 train_acc= 0.64490 val_roc= 0.63040 val_ap= 0.59745 time= 0.60480\n",
      "Epoch: 0092 train_loss= 0.61350 train_acc= 0.64633 val_roc= 0.63101 val_ap= 0.59757 time= 0.70994\n",
      "Epoch: 0093 train_loss= 0.61148 train_acc= 0.64895 val_roc= 0.63192 val_ap= 0.59779 time= 0.58341\n",
      "Epoch: 0094 train_loss= 0.60929 train_acc= 0.65207 val_roc= 0.63301 val_ap= 0.59813 time= 0.59068\n",
      "Epoch: 0095 train_loss= 0.60690 train_acc= 0.65479 val_roc= 0.63436 val_ap= 0.59857 time= 0.57398\n",
      "Epoch: 0096 train_loss= 0.60431 train_acc= 0.65834 val_roc= 0.63595 val_ap= 0.59913 time= 0.57828\n",
      "Epoch: 0097 train_loss= 0.60149 train_acc= 0.66385 val_roc= 0.63808 val_ap= 0.60001 time= 0.57690\n",
      "Epoch: 0098 train_loss= 0.59838 train_acc= 0.66792 val_roc= 0.64056 val_ap= 0.60091 time= 0.57204\n",
      "Epoch: 0099 train_loss= 0.59495 train_acc= 0.67177 val_roc= 0.64343 val_ap= 0.60222 time= 0.58561\n",
      "Epoch: 0100 train_loss= 0.59116 train_acc= 0.67817 val_roc= 0.64694 val_ap= 0.60377 time= 0.68551\n",
      "Epoch: 0101 train_loss= 0.58696 train_acc= 0.68207 val_roc= 0.65107 val_ap= 0.60563 time= 0.63655\n",
      "Epoch: 0102 train_loss= 0.58228 train_acc= 0.68450 val_roc= 0.65588 val_ap= 0.60804 time= 0.63085\n",
      "Epoch: 0103 train_loss= 0.57698 train_acc= 0.68467 val_roc= 0.66044 val_ap= 0.61046 time= 0.78613\n",
      "Epoch: 0104 train_loss= 0.56990 train_acc= 0.68918 val_roc= 0.66357 val_ap= 0.61235 time= 1.08825\n",
      "Epoch: 0105 train_loss= 0.56980 train_acc= 0.70266 val_roc= 0.65742 val_ap= 0.60886 time= 0.73296\n",
      "Epoch: 0106 train_loss= 0.56812 train_acc= 0.68419 val_roc= 0.66774 val_ap= 0.61499 time= 0.64458\n",
      "Epoch: 0107 train_loss= 0.55644 train_acc= 0.68932 val_roc= 0.66657 val_ap= 0.61369 time= 0.69678\n",
      "Epoch: 0108 train_loss= 0.56079 train_acc= 0.68386 val_roc= 0.66423 val_ap= 0.61305 time= 0.67867\n",
      "Epoch: 0109 train_loss= 0.55269 train_acc= 0.69606 val_roc= 0.66041 val_ap= 0.61096 time= 0.62738\n",
      "Epoch: 0110 train_loss= 0.55278 train_acc= 0.70280 val_roc= 0.65729 val_ap= 0.60826 time= 0.61454\n",
      "Epoch: 0111 train_loss= 0.55258 train_acc= 0.71262 val_roc= 0.65645 val_ap= 0.60812 time= 0.59555\n",
      "Epoch: 0112 train_loss= 0.54928 train_acc= 0.71460 val_roc= 0.65554 val_ap= 0.60832 time= 0.60465\n",
      "Epoch: 0113 train_loss= 0.55318 train_acc= 0.71089 val_roc= 0.65749 val_ap= 0.60863 time= 0.67119\n",
      "Epoch: 0114 train_loss= 0.54742 train_acc= 0.71384 val_roc= 0.65694 val_ap= 0.60767 time= 0.80925\n",
      "Epoch: 0115 train_loss= 0.55091 train_acc= 0.71282 val_roc= 0.66096 val_ap= 0.61088 time= 0.69328\n",
      "Epoch: 0116 train_loss= 0.54804 train_acc= 0.71006 val_roc= 0.66123 val_ap= 0.61109 time= 0.62821\n",
      "Epoch: 0117 train_loss= 0.54959 train_acc= 0.70982 val_roc= 0.65833 val_ap= 0.60868 time= 0.58415\n",
      "Epoch: 0118 train_loss= 0.54902 train_acc= 0.71266 val_roc= 0.65760 val_ap= 0.60844 time= 0.57419\n",
      "Epoch: 0119 train_loss= 0.54748 train_acc= 0.71589 val_roc= 0.65747 val_ap= 0.60897 time= 0.58224\n",
      "Epoch: 0120 train_loss= 0.54906 train_acc= 0.71435 val_roc= 0.65722 val_ap= 0.60842 time= 0.57655\n",
      "Epoch: 0121 train_loss= 0.54634 train_acc= 0.71633 val_roc= 0.65666 val_ap= 0.60749 time= 0.76950\n",
      "Epoch: 0122 train_loss= 0.54765 train_acc= 0.71784 val_roc= 0.65999 val_ap= 0.60996 time= 0.82187\n",
      "Epoch: 0123 train_loss= 0.54509 train_acc= 0.71390 val_roc= 0.66164 val_ap= 0.61112 time= 0.83200\n",
      "Epoch: 0124 train_loss= 0.54626 train_acc= 0.71182 val_roc= 0.66152 val_ap= 0.61054 time= 0.72879\n",
      "Epoch: 0125 train_loss= 0.54451 train_acc= 0.71201 val_roc= 0.66076 val_ap= 0.60995 time= 1.01160\n",
      "Epoch: 0126 train_loss= 0.54431 train_acc= 0.71363 val_roc= 0.66091 val_ap= 0.61040 time= 0.76565\n",
      "Epoch: 0127 train_loss= 0.54338 train_acc= 0.71383 val_roc= 0.66045 val_ap= 0.61014 time= 0.61869\n",
      "Epoch: 0128 train_loss= 0.54278 train_acc= 0.71475 val_roc= 0.66016 val_ap= 0.60951 time= 0.61184\n",
      "Epoch: 0129 train_loss= 0.54253 train_acc= 0.71648 val_roc= 0.66166 val_ap= 0.61049 time= 0.62761\n",
      "Epoch: 0130 train_loss= 0.54141 train_acc= 0.71479 val_roc= 0.66340 val_ap= 0.61175 time= 0.60749\n",
      "Epoch: 0131 train_loss= 0.54144 train_acc= 0.71244 val_roc= 0.66441 val_ap= 0.61213 time= 0.61088\n",
      "Epoch: 0132 train_loss= 0.54029 train_acc= 0.71149 val_roc= 0.66419 val_ap= 0.61172 time= 0.65162\n",
      "Epoch: 0133 train_loss= 0.54036 train_acc= 0.71203 val_roc= 0.66430 val_ap= 0.61198 time= 0.62520\n",
      "Epoch: 0134 train_loss= 0.53912 train_acc= 0.71288 val_roc= 0.66379 val_ap= 0.61189 time= 0.69317\n",
      "Epoch: 0135 train_loss= 0.53908 train_acc= 0.71386 val_roc= 0.66365 val_ap= 0.61153 time= 0.63605\n",
      "Epoch: 0136 train_loss= 0.53803 train_acc= 0.71538 val_roc= 0.66406 val_ap= 0.61161 time= 0.69342\n",
      "Epoch: 0137 train_loss= 0.53767 train_acc= 0.71590 val_roc= 0.66545 val_ap= 0.61271 time= 0.65381\n",
      "Epoch: 0138 train_loss= 0.53666 train_acc= 0.71392 val_roc= 0.66638 val_ap= 0.61318 time= 0.60165\n",
      "Epoch: 0139 train_loss= 0.53601 train_acc= 0.71312 val_roc= 0.66639 val_ap= 0.61293 time= 0.54609\n",
      "Epoch: 0140 train_loss= 0.53510 train_acc= 0.71449 val_roc= 0.66590 val_ap= 0.61261 time= 0.53444\n",
      "Epoch: 0141 train_loss= 0.53417 train_acc= 0.71594 val_roc= 0.66554 val_ap= 0.61268 time= 0.53614\n",
      "Epoch: 0142 train_loss= 0.53353 train_acc= 0.71631 val_roc= 0.66548 val_ap= 0.61253 time= 0.55021\n",
      "Epoch: 0143 train_loss= 0.53276 train_acc= 0.71691 val_roc= 0.66775 val_ap= 0.61360 time= 0.55931\n",
      "Epoch: 0144 train_loss= 0.53187 train_acc= 0.71470 val_roc= 0.67025 val_ap= 0.61503 time= 0.53854\n",
      "Epoch: 0145 train_loss= 0.53130 train_acc= 0.71133 val_roc= 0.66888 val_ap= 0.61424 time= 0.54569\n",
      "Epoch: 0146 train_loss= 0.53000 train_acc= 0.71500 val_roc= 0.66691 val_ap= 0.61316 time= 0.55238\n",
      "Epoch: 0147 train_loss= 0.52911 train_acc= 0.71903 val_roc= 0.66776 val_ap= 0.61373 time= 0.53727\n",
      "Epoch: 0148 train_loss= 0.52804 train_acc= 0.71896 val_roc= 0.66959 val_ap= 0.61458 time= 0.54791\n",
      "Epoch: 0149 train_loss= 0.52680 train_acc= 0.71769 val_roc= 0.67091 val_ap= 0.61525 time= 0.54333\n",
      "Epoch: 0150 train_loss= 0.52583 train_acc= 0.71710 val_roc= 0.67101 val_ap= 0.61545 time= 0.53731\n",
      "Epoch: 0151 train_loss= 0.52455 train_acc= 0.71822 val_roc= 0.66994 val_ap= 0.61483 time= 0.54609\n",
      "Epoch: 0152 train_loss= 0.52336 train_acc= 0.72135 val_roc= 0.67001 val_ap= 0.61476 time= 0.57638\n",
      "Epoch: 0153 train_loss= 0.52216 train_acc= 0.72309 val_roc= 0.67196 val_ap= 0.61589 time= 0.55949\n",
      "Epoch: 0154 train_loss= 0.52073 train_acc= 0.72113 val_roc= 0.67333 val_ap= 0.61662 time= 0.54188\n",
      "Epoch: 0155 train_loss= 0.51945 train_acc= 0.72028 val_roc= 0.67273 val_ap= 0.61619 time= 0.54647\n",
      "Epoch: 0156 train_loss= 0.51803 train_acc= 0.72330 val_roc= 0.67259 val_ap= 0.61623 time= 0.54444\n",
      "Epoch: 0157 train_loss= 0.51660 train_acc= 0.72465 val_roc= 0.67348 val_ap= 0.61672 time= 0.55236\n",
      "Epoch: 0158 train_loss= 0.51513 train_acc= 0.72450 val_roc= 0.67438 val_ap= 0.61706 time= 0.53850\n",
      "Epoch: 0159 train_loss= 0.51365 train_acc= 0.72511 val_roc= 0.67549 val_ap= 0.61771 time= 0.56615\n",
      "Epoch: 0160 train_loss= 0.51206 train_acc= 0.72443 val_roc= 0.67462 val_ap= 0.61718 time= 0.58342\n",
      "Epoch: 0161 train_loss= 0.51044 train_acc= 0.72819 val_roc= 0.67554 val_ap= 0.61772 time= 0.57785\n",
      "Epoch: 0162 train_loss= 0.50875 train_acc= 0.72869 val_roc= 0.67677 val_ap= 0.61837 time= 0.54746\n",
      "Epoch: 0163 train_loss= 0.50706 train_acc= 0.72876 val_roc= 0.67688 val_ap= 0.61839 time= 0.55375\n",
      "Epoch: 0164 train_loss= 0.50533 train_acc= 0.73124 val_roc= 0.67722 val_ap= 0.61870 time= 0.54408\n",
      "Epoch: 0165 train_loss= 0.50359 train_acc= 0.73229 val_roc= 0.67735 val_ap= 0.61862 time= 0.54182\n",
      "Epoch: 0166 train_loss= 0.50179 train_acc= 0.73614 val_roc= 0.67891 val_ap= 0.61949 time= 0.53680\n",
      "Epoch: 0167 train_loss= 0.49998 train_acc= 0.73525 val_roc= 0.67881 val_ap= 0.61942 time= 0.56465\n",
      "Epoch: 0168 train_loss= 0.49813 train_acc= 0.73973 val_roc= 0.67917 val_ap= 0.61978 time= 0.65461\n",
      "Epoch: 0169 train_loss= 0.49626 train_acc= 0.74149 val_roc= 0.67975 val_ap= 0.62022 time= 0.80722\n",
      "Epoch: 0170 train_loss= 0.49428 train_acc= 0.74364 val_roc= 0.68057 val_ap= 0.62067 time= 0.62493\n",
      "Epoch: 0171 train_loss= 0.49225 train_acc= 0.74475 val_roc= 0.68101 val_ap= 0.62096 time= 0.66770\n",
      "Epoch: 0172 train_loss= 0.49005 train_acc= 0.74666 val_roc= 0.68094 val_ap= 0.62093 time= 0.58284\n",
      "Epoch: 0173 train_loss= 0.48783 train_acc= 0.75062 val_roc= 0.68318 val_ap= 0.62242 time= 0.56524\n",
      "Epoch: 0174 train_loss= 0.48577 train_acc= 0.74768 val_roc= 0.67887 val_ap= 0.61920 time= 0.56053\n",
      "Epoch: 0175 train_loss= 0.48645 train_acc= 0.76195 val_roc= 0.68792 val_ap= 0.62616 time= 0.55912\n",
      "Epoch: 0176 train_loss= 0.49803 train_acc= 0.72555 val_roc= 0.67590 val_ap= 0.61696 time= 0.55449\n",
      "Epoch: 0177 train_loss= 0.49288 train_acc= 0.76679 val_roc= 0.67953 val_ap= 0.62029 time= 0.64322\n",
      "Epoch: 0178 train_loss= 0.48213 train_acc= 0.76330 val_roc= 0.68716 val_ap= 0.62569 time= 0.71504\n",
      "Epoch: 0179 train_loss= 0.49022 train_acc= 0.73574 val_roc= 0.68586 val_ap= 0.62434 time= 0.65775\n",
      "Epoch: 0180 train_loss= 0.47857 train_acc= 0.75459 val_roc= 0.68005 val_ap= 0.62009 time= 0.65201\n",
      "Epoch: 0181 train_loss= 0.48405 train_acc= 0.77033 val_roc= 0.68150 val_ap= 0.62207 time= 0.61872\n",
      "Epoch: 0182 train_loss= 0.47785 train_acc= 0.76101 val_roc= 0.68662 val_ap= 0.62544 time= 0.65893\n",
      "Epoch: 0183 train_loss= 0.47883 train_acc= 0.74874 val_roc= 0.68665 val_ap= 0.62508 time= 0.69538\n",
      "Epoch: 0184 train_loss= 0.47596 train_acc= 0.75551 val_roc= 0.68318 val_ap= 0.62246 time= 0.60980\n",
      "Epoch: 0185 train_loss= 0.47517 train_acc= 0.77035 val_roc= 0.68279 val_ap= 0.62302 time= 0.69056\n",
      "Epoch: 0186 train_loss= 0.47256 train_acc= 0.76356 val_roc= 0.68624 val_ap= 0.62536 time= 0.65021\n",
      "Epoch: 0187 train_loss= 0.47164 train_acc= 0.75651 val_roc= 0.68647 val_ap= 0.62532 time= 0.65599\n",
      "Epoch: 0188 train_loss= 0.46918 train_acc= 0.76252 val_roc= 0.68339 val_ap= 0.62283 time= 0.60882\n",
      "Epoch: 0189 train_loss= 0.47056 train_acc= 0.77168 val_roc= 0.68616 val_ap= 0.62538 time= 0.72232\n",
      "Epoch: 0190 train_loss= 0.46690 train_acc= 0.76089 val_roc= 0.68540 val_ap= 0.62494 time= 0.62419\n",
      "Epoch: 0191 train_loss= 0.46682 train_acc= 0.76197 val_roc= 0.68341 val_ap= 0.62309 time= 0.63425\n",
      "Epoch: 0192 train_loss= 0.46692 train_acc= 0.77232 val_roc= 0.68719 val_ap= 0.62668 time= 0.61242\n",
      "Epoch: 0193 train_loss= 0.46702 train_acc= 0.75915 val_roc= 0.68493 val_ap= 0.62446 time= 0.72245\n",
      "Epoch: 0194 train_loss= 0.46277 train_acc= 0.76862 val_roc= 0.68381 val_ap= 0.62370 time= 0.67976\n",
      "Epoch: 0195 train_loss= 0.46398 train_acc= 0.76790 val_roc= 0.68715 val_ap= 0.62641 time= 0.57875\n",
      "Epoch: 0196 train_loss= 0.46238 train_acc= 0.76299 val_roc= 0.68503 val_ap= 0.62460 time= 0.56619\n",
      "Epoch: 0197 train_loss= 0.46231 train_acc= 0.77112 val_roc= 0.68579 val_ap= 0.62514 time= 0.57169\n",
      "Epoch: 0198 train_loss= 0.46024 train_acc= 0.76974 val_roc= 0.68669 val_ap= 0.62595 time= 0.63697\n",
      "Epoch: 0199 train_loss= 0.46153 train_acc= 0.76494 val_roc= 0.68537 val_ap= 0.62470 time= 0.76224\n",
      "Epoch: 0200 train_loss= 0.45989 train_acc= 0.77068 val_roc= 0.68597 val_ap= 0.62532 time= 0.67364\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_val = []\n",
    "acc_val = []\n",
    "val_roc_score = []\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # Run single weight update\n",
    "    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_cost = outs[1]\n",
    "    avg_accuracy = outs[2]\n",
    "\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
    "    val_roc_score.append(roc_curr)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"train_acc=\", \"{:.5f}\".format(avg_accuracy), \"val_roc=\", \"{:.5f}\".format(val_roc_score[-1]),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC score: 0.678079054602\n",
      "Test AP score: 0.613249032012\n"
     ]
    }
   ],
   "source": [
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Test ROC score: ' + str(roc_score))\n",
    "print('Test AP score: ' + str(ap_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
