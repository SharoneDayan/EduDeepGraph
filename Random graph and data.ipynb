{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T09:04:46.230853Z",
     "start_time": "2017-12-28T09:04:46.215412Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import pairwise_distances, euclidean_distances\n",
    "import scipy\n",
    "import sys\n",
    "# Import T. Kipf's GCN implementation\n",
    "# https://github.com/tkipf/keras-gcn\n",
    "sys.path.append('../keras-gcn/')\n",
    "\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:00.154656Z",
     "start_time": "2017-12-28T10:43:00.152229Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes = 2500\n",
    "n_features = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:00.910144Z",
     "start_time": "2017-12-28T10:43:00.395274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# X : (nodes, n_features) random features matrix of 0/1\n",
    "X = np.matrix(np.random.choice([0,1], (nodes, n_features)))\n",
    "# A : (nodes, nodes) random adjacency matrix\n",
    "A = scipy.sparse.rand(nodes, nodes, density=0.4, format='csr')\n",
    "A.data[:] = 1\n",
    "# (nodes, 1) vector with the category of each row\n",
    "y = np.random.choice([0,1], nodes).reshape((nodes, 1))\n",
    "# Converts a class vector to binary class matrix.\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:00.915039Z",
     "start_time": "2017-12-28T10:43:00.911549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500, 600), (2500, 2500))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:00.919775Z",
     "start_time": "2017-12-28T10:43:00.916562Z"
    }
   },
   "outputs": [],
   "source": [
    "# split for training/validation and testing\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:01.129448Z",
     "start_time": "2017-12-28T10:43:01.115799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize X \n",
    "X = X/X.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:01.741771Z",
     "start_time": "2017-12-28T10:43:01.619312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Local Pooling\n",
    "# Preprocessing A: multiplication with A means that, for every node, we sum up all the feature vectors \n",
    "# of all neighboring nodes but not the node itself (unless there are self-loops in the graph). \n",
    "# We can \"fix\" this by enforcing self-loops in the graph: we simply add the identity matrix to A.\n",
    "A_ = preprocess_adj(A, True)\n",
    "support = 1\n",
    "graph = [X, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:02.367587Z",
     "start_time": "2017-12-28T10:43:02.265133Z"
    }
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:13.573550Z",
     "start_time": "2017-12-28T10:43:03.026531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.6931 train_acc= 0.5571 val_loss= 0.6932 val_acc= 0.4667 time= 1.6878\n",
      "Epoch: 0002 train_loss= 0.6931 train_acc= 0.5571 val_loss= 0.6932 val_acc= 0.4667 time= 0.8144\n",
      "Epoch: 0003 train_loss= 0.6931 train_acc= 0.5571 val_loss= 0.6932 val_acc= 0.4667 time= 0.8450\n",
      "Epoch: 0004 train_loss= 0.6930 train_acc= 0.5571 val_loss= 0.6932 val_acc= 0.4667 time= 0.8042\n",
      "Epoch: 0005 train_loss= 0.6930 train_acc= 0.5571 val_loss= 0.6932 val_acc= 0.4667 time= 0.7844\n",
      "Epoch: 0006 train_loss= 0.6929 train_acc= 0.5571 val_loss= 0.6933 val_acc= 0.4667 time= 0.7812\n",
      "Epoch: 0007 train_loss= 0.6928 train_acc= 0.5571 val_loss= 0.6933 val_acc= 0.4667 time= 0.7998\n",
      "Epoch: 0008 train_loss= 0.6926 train_acc= 0.5571 val_loss= 0.6935 val_acc= 0.4667 time= 0.7701\n",
      "Epoch: 0009 train_loss= 0.6922 train_acc= 0.5571 val_loss= 0.6938 val_acc= 0.4667 time= 0.7850\n",
      "Epoch: 0010 train_loss= 0.6917 train_acc= 0.5571 val_loss= 0.6941 val_acc= 0.4667 time= 0.7654\n",
      "Epoch: 0011 train_loss= 0.6911 train_acc= 0.5571 val_loss= 0.6947 val_acc= 0.4667 time= 0.7787\n",
      "Epoch: 0012 train_loss= 0.6904 train_acc= 0.5571 val_loss= 0.6953 val_acc= 0.4667 time= 0.8837\n",
      "Epoch 12: early stopping\n",
      "Test set results: loss= 0.6938 accuracy= 0.4950\n"
     ]
    }
   ],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10 # early stopping patience\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "\"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:43:16.718699Z",
     "start_time": "2017-12-28T10:43:16.714659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51364863,  0.48635143],\n",
       "       [ 0.51326466,  0.48673531],\n",
       "       [ 0.51361322,  0.48638678],\n",
       "       ..., \n",
       "       [ 0.51359695,  0.48640305],\n",
       "       [ 0.51323998,  0.48675999],\n",
       "       [ 0.51386386,  0.48613608]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X contains randomly NaN values, what can we do to no have NaN predictions and a Nan loss ?  \n",
    "We see two to face this issue :\n",
    "- Fill NaN with 0.5 or the columns mean \n",
    "- Fill NaN with 0 and translate the other values (NaN->0, 0->1, 1->2) (https://www.kaggle.com/c/predict-west-nile-virus/discussion/13725#74831)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:07:26.452378Z",
     "start_time": "2017-12-28T11:07:26.449882Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes = 2500\n",
    "n_features = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First method for NaN : Translation and filling NaN with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:16:48.854682Z",
     "start_time": "2017-12-28T11:16:48.385122Z"
    }
   },
   "outputs": [],
   "source": [
    "# We randomly add nan values to X\n",
    "X = np.matrix(np.random.choice([0,1, np.nan], (nodes, n_features)))\n",
    "A = scipy.sparse.rand(nodes, nodes, density=0.4, format='csr')\n",
    "A.data[:] = 1\n",
    "# (nodes, 1) vector with the success\n",
    "y = np.random.choice([0,1], nodes).reshape((nodes, 1))\n",
    "# Converts a class vector to binary class matrix.\n",
    "y = to_categorical(y)\n",
    "\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:52:56.002823Z",
     "start_time": "2017-12-28T10:52:55.968577Z"
    }
   },
   "outputs": [],
   "source": [
    "# translation\n",
    "X = X+1\n",
    "\n",
    "# Normalize X \n",
    "X = X/np.nansum(X,1).reshape(-1, 1)\n",
    "\n",
    "# filling NaN with 0s\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:40:56.083299Z",
     "start_time": "2017-12-28T10:40:55.951510Z"
    }
   },
   "outputs": [],
   "source": [
    "# local pool\n",
    "A_ = preprocess_adj(A, True)\n",
    "support = 1\n",
    "graph = [X, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:40:57.989945Z",
     "start_time": "2017-12-28T10:40:57.879049Z"
    }
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:42:07.147822Z",
     "start_time": "2017-12-28T10:41:45.784005Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7760\n",
      "Epoch: 0002 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7983\n",
      "Epoch: 0003 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7675\n",
      "Epoch: 0004 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7971\n",
      "Epoch: 0005 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7507\n",
      "Epoch: 0006 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7613\n",
      "Epoch: 0007 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6906 val_acc= 0.5433 time= 0.7634\n",
      "Epoch: 0008 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.7934\n",
      "Epoch: 0009 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.8393\n",
      "Epoch: 0010 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.8770\n",
      "Epoch: 0011 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7502\n",
      "Epoch: 0012 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7795\n",
      "Epoch: 0013 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.7835\n",
      "Epoch: 0014 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6903 val_acc= 0.5433 time= 0.7941\n",
      "Epoch: 0015 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6903 val_acc= 0.5433 time= 0.8386\n",
      "Epoch: 0016 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.8117\n",
      "Epoch: 0017 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6906 val_acc= 0.5433 time= 0.8213\n",
      "Epoch: 0018 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6906 val_acc= 0.5433 time= 0.8093\n",
      "Epoch: 0019 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.8386\n",
      "Epoch: 0020 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.8557\n",
      "Epoch: 0021 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6903 val_acc= 0.5433 time= 0.8441\n",
      "Epoch: 0022 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.8881\n",
      "Epoch: 0023 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6906 val_acc= 0.5433 time= 0.8920\n",
      "Epoch: 0024 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6905 val_acc= 0.5433 time= 0.8591\n",
      "Epoch: 0025 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6904 val_acc= 0.5433 time= 0.9314\n",
      "Epoch: 0026 train_loss= 0.6923 train_acc= 0.5214 val_loss= 0.6903 val_acc= 0.5433 time= 0.8916\n",
      "Epoch 26: early stopping\n",
      "Test set results: loss= 0.6936 accuracy= 0.5050\n"
     ]
    }
   ],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10 # early stopping patience\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] <= best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "\"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T10:42:12.679830Z",
     "start_time": "2017-12-28T10:42:12.676129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47942787,  0.52057219],\n",
       "       [ 0.48045942,  0.51954055],\n",
       "       [ 0.47922593,  0.52077407],\n",
       "       ..., \n",
       "       [ 0.47943738,  0.52056259],\n",
       "       [ 0.48083887,  0.51916116],\n",
       "       [ 0.48068473,  0.5193153 ]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:08:06.046782Z",
     "start_time": "2017-12-28T11:08:06.043229Z"
    }
   },
   "source": [
    "#### Second method for NaN : filling NaN with column means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly add nan values to X\n",
    "X = np.matrix(np.random.choice([0,1, np.nan], (nodes, n_features)))\n",
    "A = scipy.sparse.rand(nodes, nodes, density=0.4, format='csr')\n",
    "A.data[:] = 1\n",
    "# (nodes, 1) vector with the success\n",
    "y = np.random.choice([0,1], nodes).reshape((nodes, 1))\n",
    "# Converts a class vector to binary class matrix.\n",
    "y = to_categorical(y)\n",
    "\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:10:16.992605Z",
     "start_time": "2017-12-28T11:10:16.954760Z"
    }
   },
   "outputs": [],
   "source": [
    "col_mean = np.nanmean(X, axis=0)\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(X))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "X[inds] = np.take(col_mean, inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:10:34.374389Z",
     "start_time": "2017-12-28T11:10:34.243208Z"
    }
   },
   "outputs": [],
   "source": [
    "# local pool\n",
    "A_ = preprocess_adj(A, True)\n",
    "support = 1\n",
    "graph = [X, A_]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:10:39.863070Z",
     "start_time": "2017-12-28T11:10:39.754051Z"
    }
   },
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define model architecture\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', W_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:11:01.150496Z",
     "start_time": "2017-12-28T11:10:44.727543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.3742 train_acc= 0.4857 val_loss= 1.2077 val_acc= 0.5533 time= 1.4730\n",
      "Epoch: 0002 train_loss= 0.8755 train_acc= 0.4857 val_loss= 0.7973 val_acc= 0.5533 time= 0.8854\n",
      "Epoch: 0003 train_loss= 0.7302 train_acc= 0.5143 val_loss= 0.7706 val_acc= 0.4467 time= 0.8353\n",
      "Epoch: 0004 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.7949\n",
      "Epoch: 0005 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8277\n",
      "Epoch: 0006 train_loss= 0.6930 train_acc= 0.5143 val_loss= 0.6937 val_acc= 0.4467 time= 0.8052\n",
      "Epoch: 0007 train_loss= 0.8060 train_acc= 0.5143 val_loss= 0.8744 val_acc= 0.4467 time= 0.7911\n",
      "Epoch: 0008 train_loss= 0.6934 train_acc= 0.4857 val_loss= 0.6922 val_acc= 0.5533 time= 0.8231\n",
      "Epoch: 0009 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8885\n",
      "Epoch: 0010 train_loss= 0.6933 train_acc= 0.4857 val_loss= 0.6925 val_acc= 0.5533 time= 0.8608\n",
      "Epoch: 0011 train_loss= 0.6931 train_acc= 0.5143 val_loss= 0.6933 val_acc= 0.4467 time= 0.8005\n",
      "Epoch: 0012 train_loss= 0.7028 train_acc= 0.5143 val_loss= 0.7254 val_acc= 0.4467 time= 0.7778\n",
      "Epoch: 0013 train_loss= 0.6928 train_acc= 0.5143 val_loss= 0.6967 val_acc= 0.4467 time= 0.7800\n",
      "Epoch: 0014 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8290\n",
      "Epoch: 0015 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.7896\n",
      "Epoch: 0016 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8367\n",
      "Epoch: 0017 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8158\n",
      "Epoch: 0018 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.9065\n",
      "Epoch: 0019 train_loss= 0.6931 train_acc= 0.4857 val_loss= 0.6931 val_acc= 0.5533 time= 0.8489\n",
      "Epoch 19: early stopping\n",
      "Test set results: loss= 0.6931 accuracy= 0.4640\n"
     ]
    }
   ],
   "source": [
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10 # early stopping patience\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] <= best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "\"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T11:11:03.780823Z",
     "start_time": "2017-12-28T11:11:03.775539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5],\n",
       "       [ 0.5,  0.5],\n",
       "       [ 0.5,  0.5],\n",
       "       ..., \n",
       "       [ 0.5,  0.5],\n",
       "       [ 0.5,  0.5],\n",
       "       [ 0.5,  0.5]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
